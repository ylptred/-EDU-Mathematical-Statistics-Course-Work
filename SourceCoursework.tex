\documentclass[14pt,a4paper,oneside]{extbook}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{xparse}
\usepackage{listings}
\usepackage{longtable}
\usepackage{tabto}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{alltt}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hyperref}
\usepackage{datetime}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{graphicx}
\newcolumntype{L}[1]{>{\hsize=#1\hsize\raggedright\arraybackslash}X}%
\newcolumntype{R}[1]{>{\hsize=#1\hsize\raggedleft\arraybackslash}X}%
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}%

\newcommand{\T}{\rule{0pt}{2.3ex}} %top strut
\newcommand{\B}{\rule[-1.2ex]{0pt}{0ex}} %bottom strut

\newcommand{\sexstar}{{\fontfamily{lmr}\selectfont$\star$}}
\newcommand*{\expect}{\mathsf{M}}
\newcommand*{\prob}{\mathsf{P}}

\usepackage[section, above, below]{placeins}

\usepackage{titlesec}
\setcounter{tocdepth}{2}
\newcommand{\chn}{{{}{\arabic{chapter}.}}}
\titleformat{\chapter}[block]{\bfseries\raggedright\hyphenpenalty=2000}
{\LARGEДомашнее задание \arabic{chapter}.}{1ex}{\rule{\linewidth}{1pt}\newline\Large}

\titleformat{\section}[block]{\bfseries\raggedright\hyphenpenalty=10000}
{\arabic{section}.}{1ex}{}
\titleformat{\subsection}[block]{\bfseries\normalsize\raggedright\hyphenpenalty=10000}
{\chn\arabic{section}.\arabic{subsection}.}{1ex}{}
\titleformat{\subsubsection}[block]{\bfseries\normalsize\raggedright\hyphenpenalty=10000}
{\chn\arabic{section}.\arabic{subsection}.\arabic{subsubsection}.}{1ex}{}
\addto\captionsrussian{\renewcommand{\chaptername}{Р—Р°РґР°РЅРёРµ}}
\titlespacing*{\chapter}      {0pt}{3.50ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titlespacing*{\section}      {0pt}{3.50ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titlespacing*{\subsection}   {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

%\sloppy

\renewcommand{\labelitemi}{$\triangleright$}



\newcounter{rownum}
\setcounter{rownum}{0}
\newcommand{\Rownum}{\stepcounter{rownum}%
	\arabic{rownum}}


\usepackage{catchfile,environ,tikz}

\newcounter{tablinegen}
\newcounter{tabrow}
\newcommand{\nl}{\stepcounter{tabrow}\\ \hline}
\newcommand{\blankline}{\thetabrow & \nl}
\newcommand{\blanklist}[3][.8\textwidth]{% #1 - names
	\begingroup
	\setcounter{tablinegen}{0}
	\let\tablines\empty%
	\loop\ifnum\thetablinegen<#2
	\stepcounter{tablinegen}
	\expandafter\def\expandafter\tablines\expandafter{%
		\tablines
		\blankline
	}%
	\repeat
	\blanklistformat{#1}}
\newcommand{\blanklistformat}[1]{
	\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|}
		\hline
		Р¤РРћ & Р”РёСЃРєСЂРµС‚РЅРѕРµ СЂР°СЃРїСЂРµРґРµР»РµРЅРёРµ & РќРµРїСЂРµСЂС‹РІРЅРѕРµ СЂР°СЃРїСЂРµРґРµР»РµРЅРёРµ  \nl
		\tablines
	\end{tabular}
	\endgroup
	\setcounter{tabrow}{0}}


\usepackage{catchfile,environ,tikz}


\begin{document}
	
	
	\begin{titlepage}                                                         
		\newpage                                                                        
		\begin{center}                                                        
			{\bfseries Национальный Исследовательский Университет \\
				Высшая Школа Экономики \\
				Московский Институт Электроники и Математики}                               
			\vspace{1cm}                                                          
			%РЎРђРќРљРў-РџР•РўР•Р Р‘РЈР Р“РЎРљРР™ \\*                                                
			%Р“РћРЎРЈР”РђР РЎРўР’Р•РќРќР«Р™ РЈРќРР’Р•Р РЎРРўР•Рў \\*                                        
			%\hrulefill                                                                     
			%\end{center}                                                         
			
			%{РљРђР¤Р•Р”Р Рђ РЇР”Р•Р РќРћР™ Р¤РР—РРљР }                                 
			Департамент Прикладной Математики
			
			Кафедра Компьютерной Безопасности                                                              
			\vspace{6em}                                                          
			
			
			
			
			
			
			
		\end{center}                                                          
		
		\vspace{1.2em}                                                        
		
		\begin{center}                                                        
			%\textsc{\textbf{}}                                     
			\Large Долгосрочное домашнее задание по математической
			статистике\linebreak                                  
			
			
		\end{center}                                            
		\begin{center}
			\textbf{Дискретное распределение:}\textit{ Биномиальное распределение $n = 87, \theta = 0.6$}\\
			\textbf{Непрерывное распределение:} \textit{ Распределение Максвелла $\theta = 3$}
		\end{center}
		
		\vspace{5em}                                                          
		
		\begin{center}                                                        
			%\Large                                                                         
			
		\end{center}                                                         
		\vspace{6em}                                                          
		
		%\begin{center}                                                   
		%\begin{tabbing}                                                      
		
		\quad\tabto{320pt}Выполнил \\                                     
		\tabto{320pt}Смирнов Д. А.\\                                           
		\vspace{1.2em}                                                       
		\tabto{320pt}Проверил \\                                                      
		\tabto{320pt}Чухно А. Б.\\                                        
		%\end{tabbing}                                                        
		%\end{center}                                                         
		\begin{comment}                          
		\begin{alltt}                                                         
		РќР°СѓС‡РЅС‹Р№ СЂСѓРєРѕРІРѕРґРёС‚РµР»СЊ                                             
		Рґ.С„.Рј.РЅ., Andrey Р’.Рђ.                                            
		Р РµС†РµРЅР·РµРЅС‚                                                        
		Рє.С„.-Рј.РЅ. Olegovich Р’.Р.                                         
		\end{alltt}                                             
		\end{comment}                                                                                        
		
		\vspace{\fill}                                                    
		
		\begin{center}                                                        
			Москва \the\year{}                                                                
		\end{center}                                                          
	\end{titlepage}
	\setcounter{page}{2}
	\tableofcontents
	
	\chapter{Характеристики вероятностных распределений}
	
	\underline{\textbf{Описание основных характеристик распределений}}
	\section{Биномиальное распределение}
	$\displaystyle P(x) = \binom{n}{x}\theta^{x}(1-\theta)^{n-x}, x \in \{0, 1, ..., n\}, n \in \mathbb {N}, 0 < \theta < 1$ 
	
	\subsection{Функция Распределения}
	В общем виде функция распределения Биномиального распределения выглядит следующим образом [\textit{\textbf{1} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности", МИЭМ НИУ ВШЭ, Москва 2022: \textbf{стр. 90}}]: \\
	$F(x) = P(\xi < x) = \displaystyle\sum_{k=0}^{[x]} \binom{n}{k}\theta^{k}(1-\theta)^{n-k}, x \in \{0,1,...,n\}, n \geq 1, 0 < \theta < 1$
	
	\subsection{Математическое ожидание}
	По определению, математическое ожидание дискретной случайной величины вычисляется по формуле [\textit{\textbf{2} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 63}}]: \\
	$\displaystyle M\xi = \displaystyle\sum_{i\geq1}x_i \cdot p_i$, где $p_i = P(\xi = x_i)$ \\
	Пусть случайная величина $\xi$ имеет биномиальное распределение с параметрами $(n, \theta)$, что соответствует числу успехов в $n$ независимых испытаниях Бернулли с вероятностю успеха $\theta$ в каждом испытании [\textit{\textbf{3} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 67}}]. \\
	Представим $\xi$ в виде суммы $n$ независимых индикаторов $\chi_1,...,\chi_n$, где $\chi_i = 1$, если в $i$-ом испытании Бернулли произошел успех. \\
	Имеем: $\displaystyle P(\chi_i = 1) = \theta, P(\chi_i = 0) = 1 - \theta, i \in \overline{1,n}$ \\
	Тогда: $\displaystyle M\xi = M(\displaystyle\sum_{i = 1}^{n}\chi_i) = \displaystyle\sum_{i=1}^{n}M\chi_i = n\theta$
	
	\subsection{Дисперсия}
	По определению, Дисперсия $D\xi = M(\xi - M\xi)^2 = M\xi^2 - (M\xi)^2$ [\textit{\textbf{4} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 71}}] \\
	Также по \textit{\textbf{Определению 7.5}} [\textit{\textbf{5} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 71}}] знаем, что для дискретной случайной величины \\
	$\displaystyle M(\xi - M\xi)^{n} = \displaystyle\sum_{k=1}^{n}(x_k - M\xi)^{n} \cdot P(\xi = x_k)$ \\
	Тогда: $D\xi = \displaystyle\sum_{k = 1}^{n=2}(\chi_k - M\xi)^2 \cdot p_k$. С учетом функции распределения из пункта \textbf{1.1.1}, получаем, что \\
	$D\xi = \displaystyle\sum_{k = 1}^{n=2}(\chi_k - M\xi)^2 \cdot p_k = \displaystyle\sum_{x=0}^{n=2}(x - n \cdot \theta)^{2} \cdot \binom{n}{x} \cdot \theta^{x} \cdot (1 - \theta)^{n - x} = n \cdot \theta \cdot (1 - \theta)$
	
	\subsection{Квантиль уровня $\gamma$}
	$\gamma$-квантиль - это такое значение $\chi$ случайной величины $\Chi$, для которого $P(\Chi \leq \chi \cdot \gamma) = \gamma$. То есть, вероятность того, что случайная величина $\Chi$ примет значение меньше или равное $x$ [\textit{\textbf{6} - \href{https://allatambov.github.io/psms/pdf/quantiles.pdf}{Источник}}]\\
	$\displaystyle\sum_{k=0}^{[x]} \binom{n}{k} \theta^{k} \cdot (1 - \theta)^{n - k} = \gamma$
	
	\subsection{Пример интерпритации распределения}
    Биномиальное распределение - распределение количества «успехов» в последовательности из $n$ независимых случайных экспериментов, таких что вероятность «успеха» в каждом из них равна $\theta$. [\textit{\textbf{7} - \href{https://ru.wikipedia.org/wiki/Биномиальное_распределение}{Источник}}] Такая схема испытаний (экспериментов) называется схемой испытаний Бернулли. [\textit{\textbf{8} - \href{https://ru.wikipedia.org/wiki/Схема_Бернулли}{Источник}}]\\ 
    Примеры:
    \begin{enumerate}
        \item \textbf{Контроль качества изделий} [\textit{\textbf{9} - \href{https://studopedia.ru/5_19612_primenenie-binomialnogo-raspredeleniya.html}{Источник}}] \\
        Каждое изделие с вероятностью $\theta$ может быть дефектным. Появление как дефектных, так и стандартных изделий происходит независимо друг от друга. \\
        \textit{Пример:} \\
        Дано: $n = 87 изделий, \theta = 0.6 - $ вероятность того, что изделие "стандартно" \\
        Решение: Случайная величина $\xi$ распределена по Биномиальному закону с вышеописанными параметрами. Воспользуемся формулой Бернулли: \\
        $\displaystyle P(\xi = x) = P_{n}(x) = \binom{n}{x}\theta^{x}(1-\theta)^{n - x} = \binom{87}{x}0.6^{x}(1-0.6)^{87 - x} = \frac{87!}{x!(87 - x)!} \cdot 0.6^{x} \cdot 0.4^{87 - x}, x = 0, 1, 2, \cdots, 87$ \\
        С помощью скрипта на языке Python составим таблицу, описывающую ряд распределения: \\
        \begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=1]{HW1/Binomial/Examples/Details/1.png}
        \includegraphics[scale=1]{HW1/Binomial/Examples/Details/2.png}}
        \caption{}
        \label{fig:my_label}
    \end{figure}
        \newpage
        \item \textbf{Телекоммуникации} [\textit{\textbf{10} - \href{http://statistica.ru/theory/binomialnoe-raspredelenie/}{Источник}}] \\
        Здесь $(1 - \theta)$ - доля необслуженных вызовов. \\
        Пример для данной интерпритации анологичен примеру, описанному выше.
    \end{enumerate}
    \subsection{Соотношения между распределениями} [\textit{\textbf{11} - \href{https://ru.wikipedia.org/wiki/Биномиальное_распределение#Связь_с_другими_распределениями}{Источник}}]
    \begin{enumerate}
        \item Если $n = 1$, то получаем распределение Бернулли.
        \item Если $n$ большое, то в силу центральной предельной теоремы [\textit{\textbf{12} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 124}}] \\ $Bin(n, \theta) \longrightarrow_{n \longrightarrow \infty}^{p} N(n\theta, n\theta(1-\theta))$, где $N(n\theta, n\theta(1-\theta))$ - нормальное распределение с $M\xi = n\theta$ и $D\xi = n\theta(1-\theta)$.
        \item Если $n$ - большое, а $\lambda$ - фиксированное число, то $Bin(n, \frac{\lambda}{n}) \longrightarrow_{n \longrightarrow \infty}^{p} P(\lambda)$, где $P(\lambda)$ - распределение Пуассона с параметром $\lambda$. 
    \end{enumerate}
    
    \subsection{Описание способа моделирования выбранных случайных величин}
    [\textit{\textbf{13} - \href{http://statmod.ru/wiki/_media/books:vv:simulation_v4.pdf}{Источник, \textbf{стр. 26}}}] Биномиальное распределение $Bin(n, \theta)$ с параметрами $n \in \mathbb{N} \textrm{и} \theta \in (0,1)$ можно задать с помощью \textbf{таблицы распределения:} \\
    $
        P:
        \begin{pmatrix}
            0 & 1 & \cdots & k & \cdots & n \\
            \theta_{0} & \theta_{1} & \cdots & \theta_{k} & \cdots & \theta_{n}
        \end{pmatrix}
    $
	, где $\theta_{k} = \binom{n}{k} \theta^{k} (1 - \theta)^{n - k}.$ \\
	Воспользуемся последовательным методом набором обратных функций в нетабличном варианте. Накопленные вероятности $s_{k}$ здесь вычисляются в том же цикле, где проверяются неравенства $\alpha \leq s_{k}$. То есть вероятности $\theta_{k}$ рекуррентно пересчитываются одна через другую, а накопленные вероятности $s_{k}$ последовательно вычисляются через $s_{k-1} \textrm{и} \theta_{k}$ \\
	Для биномиального распределения $\theta_{0} = (1 - \theta)^{n} \textrm{ и }$ \\
	\begin{center}
	$\displaystyle\frac{\theta_{k}}{\theta_{k-1}} = \frac{n - k + 1}{k} \frac{\theta}{1 - \theta}$
	\end{center}
	\\
	при $k = 1, 2, \cdots, n, \textrm{то мы приходим к алгоритму BIS (Binomial Inverse Sequential).}$ Однако, воспользуемся мы алгоритмом BISM: заметим, что если $\xi \in Bin(n, \theta), \\ \textrm{ то } \eta = n - \xi \in Bin(n, 1 - \theta).$ Поэтому, если $\theta > 0.5$, то можно применить алгоритм BIS к моделированию числа неудач в $n$ испытаниях Бернулли с вероятностью
    успеха $\theta$, а потом перейти к числу успехов, что равносильно применению последовательного метода обратных функций «справа налево», а не «слева направо». \\
    Далее описан \textbf{Алгоритм BISM (Binomial Inverse Sequential Modified)}: \\
    \begin{center}
	Моделирование $Bin(n, \theta)$ модифицированным последовательным методом обратных функций.
	\end{center} \\
	Входные данные: $n, \theta$ \\
	Результат: $\xi$. \\
	\textbf{1. Инициализация}
	\begin{itemize}
	    \item $\textrm{If } \theta \leq 0.5 \textrm{ then } t \longleftarrow \theta \textrm{ else } t \longleftarrow 1 - \theta;$
	    \item $c \longleftarrow t/(1 - t); s \longleftarrow r \longleftarrow (1 - t)^{n}; k \longleftarrow 0; Get(\alpha);$
	\end{itemize}
	\\
	\textbf{2. Пересчет вероятностей и поиск окна}
	\begin{itemize}
	    \item $k \longleftarrow k + 1; r \longleftarrow r \cdot c \cdot (n - k + 1)/k; s \longleftarrow s + r;$
	\end{itemize}
	\\
	\textbf{2. Завершение: } $\textrm{If } p \leq 0.5 \textrm{ then } \xi \longleftarrow k \textrm{ else } \xi \longleftarrow n - k; \textrm{ STOP. }$
	\\
	Комментария требует лишь введение переменной $t$, которая необходима при завершении алгоритма, чтобы (если нужно) перейти от моделирования числа неудач в испытаниях Бернулли к
    моделированию числа успехов. \\
    В процессе выполнения Домашнего Задания мною была разработана программа на языке Python, моделирующая 10000 случайных величин с Биномиальным распределением и строющая график функции вероятности. Параметры: $n = 87, \theta = 0.6$ \\
    \textbf{\textit{График:}}
    \begin{center}
         \includegraphics[scale=1.2]{HW1/Binomial/3_Model/Binomial.png}
    \end{center}

    \\
    \textbf{\textit{Листинг кода:}}
    \lstinputlisting[language=Python]{HW1/Binomial/3_Model/binomial.py}
	\newpage
	\section{Распределение Максвелла}
	$\displaystyle f(x) = \sqrt{\frac{2}{\pi}} \frac{x^{2}}{\theta^{3}} e^{-\frac{x^{2}}{2\theta^{2}}}, x, \theta \in \mathbb{R^{+}}$
	\subsection{Функция Распределения}
	По определению [\textit{\textbf{14} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 52}}], и, учитывая, что для распределения Максвелла $x, \theta \in \mathbb{R^{+}}$, функция распределения вычисляется по формуле: \\
	$\displaystyle F(x) = \int\limits_{0}^{x}f(t)\mathrm{d}t$. Тогда:

    $F_{\xi}(x) = \int\limits_{0}^{x} f(t) \,\mathrm{d}t = \int\limits_{0}^{x} \displaystyle \sqrt{\frac{2}{\pi}}\frac{t^2}{\theta^3} e^{-\frac{t^2}{2\theta^2}}\,\mathrm{d}t = \displaystyle \sqrt{\frac{2}{\pi}}\frac{1}{\theta^3} \int\limits_{0y}^{x} t^2 e^{-\frac{t^2}{2\theta^2}}\,\mathrm{d}t  =$ \\ опираясь на то, что $d(e^{-\frac{t^2}{2\theta^2}})= -\frac{t e^{-\frac{t^2}{2\theta^2}}}{\theta^2}\,\mathrm{d}t,$ получим $= 
    \displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta^3}\theta^2 \int\limits_{0}^{x} \frac{t^2}{t}\,d(e^{-\frac{t^2}{2\theta^2}}) = \displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta} \int\limits_{0}^{x}t\,d(e^{-\frac{t^2}{2\theta^2}}) =$ возьмем интеграл по частям $= \displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta}(t e^{-\frac{t^2}{2\theta^2}}\bigg|_{-\infty}^{x} -- \int\limits_{0}^{x} e^{-\frac{t^2}{2\theta^2}}\,\mathrm{d}t) = \displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta}x e^{-\frac{x^2}{2\theta^2}} + \sqrt{\frac{2}{\pi}}\frac{1}{\theta}\theta\sqrt{2\pi}\int\limits_{0}^{x} \frac{1}{\theta\sqrt{2\pi}}e^{-\frac{t^2}{2\theta^2}}\,\mathrm{d}t) =$\\ видно, что правое слагаемое есть функция плотности стандартного нормального распределения $ - \frac{1}{2}$ $= \displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta}x e^{-\frac{x^2}{2\theta^2}} + 2\int\limits_{0}^{x} f_N(t)\,\mathrm{d}t  =\displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta}x e^{-\frac{x^2}{2\theta^2}} + \\ + 2 F_N(x)$
    
    Здесь $F_N(x)$ - функция распределения N(0,1).\vspace{0.7in}
    
	\subsection{Математическое ожидание}
	По определению [\textit{\textbf{15} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 68}}], и, учитывая, что для распределения Максвелла $x, \theta \in \mathbb{R^{+}}$, математическое ожидание вычисляется по формуле: \\
	$M\xi = \int_{0}^{+\infty}x\mathrm{d}F_{\xi}(x)$. Тогда: \\
	\begin{multline*}
    M\xi = \int\limits_\mathbb{R} x f\xi(x)\mathrm{d}x= \int\limits_{0}^{+\infty}x \sqrt{\frac{2}{\pi}} \frac{x^2}{\theta^3}\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x = \sqrt{\frac{2}{\pi}} \frac{1}{\theta^3} \int\limits_{0}^{+\infty} x^3\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x =\\= \sqrt{\frac{2}{\pi}} \frac{1}{\theta^3} \int\limits_{0}^{+\infty} x^3\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x \end{multline*}
    Для упрощения работы, сначала вычислим неопределённый интеграл:
    \begin{multline*}
    \int x^3\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x = \textrm{Произведём замену вида } u = x^{2}, \textrm{ тогда } \mathrm{d}x = \frac{1}{2x}\mathrm{d}u = \\ = \frac{1}{2} \int u\exp(-\frac{u}{2\theta^2})\mathrm{d}u = \textrm{Произведём замену вида } v = -\frac{u}{2\theta^2}, \textrm{ тогда } \mathrm{d}u = -2\theta^2 d v =\\= 2\theta^4 \int v e^v \mathrm{d}v = \textrm{Возьмём интеграл по частям} =2\theta^4 (v e^v - \int e^v \mathrm{d}v) = 2\theta^4 (v e^v -  e^v) = \\ = \textrm{Выполним обратную замену} = -(\theta^2x^2 + 2\theta^4)\exp(-\frac{x^2}{2\theta^2}) + \mathbb{C}
    \end{multline*}
    Таким образом:
    $$-(\theta^2x^2 + 2\theta^4)\exp(-\frac{x^2}{2\theta^2}) \bigg|_{0}^{+\infty}  = 2\theta^4$$
    Следовательно, математическое ожидание равно: $\displaystyle \sqrt{\frac{2}{\pi}} \frac{1}{\theta^3}  2\theta^4 = \displaystyle\sqrt{\frac{2}{\pi}} 2\theta $\\
    
	\subsection{Дисперсия}
	По определению, Дисперсия $D\xi = M(\xi - M\xi)^2 = M\xi^2 - (M\xi)^2$ [\textit{\textbf{16} - Фомин Д. Б, Чухно А. Б., "Теория вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 71}}] \\
	В пункте \textbf{1.2.2} было выведено $M\xi = \displaystyle\sqrt{\frac{2}{\pi}} 2\theta$, тогда $(M\xi)^2 = \displaystyle \frac{8}{\pi} \theta^2$.\\
    Выведем $M\xi^2$:
    \begin{multline*}
    M\xi^2 = \int\limits_\mathbb{R} x^2 f\xi(x)\mathrm{d}x = \int\limits_{0}^{+\infty}x^2 \sqrt{\frac{2}{\pi}} \frac{x^2}{\theta^3}\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x = \sqrt{\frac{2}{\pi}} \frac{1}{\theta^3} \int\limits_{0}^{+\infty} x^4\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x =\\= \sqrt{\frac{2}{\pi}} \frac{1}{\theta^3} \int\limits_{0}^{+\infty} x^4\exp(-\frac{x^2}{2\theta^2})\mathrm{d}x =\end{multline*}
    = это следующий интеграл
    \begin{multline*}
     \int\limits_{0}^{+\infty} x^n\exp(-\alpha x^2)\mathrm{d}x = 
    \begin{cases}
    \displaystyle\frac{(2k-1)!!}{2^{k+1}\alpha^k}\sqrt{\frac{\pi}{\alpha}} & \textrm{если } n = 2k, k \in \mathbb{N} , \alpha > 0\\
    \displaystyle\frac{k!}{2\alpha^{k+1}}  & \textrm{если } n = 2k+1, k \in \mathbb{N} , \alpha > 0 
    \end{cases}
      =\\=  \sqrt{\frac{2}{\pi}} \frac{1}{\theta^3} \frac{3}{8} \sqrt{\pi} (2\theta^2)^{2.5}  = 3\theta^2
    \end{multline*}
    Таким образом, получаем дисперсию:
    $$D\xi = M(\xi-M\xi)^2 = M\xi^2 - (M\xi)^2 = 3\theta^2 - \displaystyle \frac{8}{\pi} \theta^2 = \theta^2 (3 - \displaystyle\frac{8}{\pi})$$\\
    
	\subsection{Квантиль уровня $\gamma$}
	По определению: $F\xi(x_\gamma) = \gamma$, значит квантиль распределения - решение уравнения 
    $$\displaystyle -\sqrt{\frac{2}{\pi}}\frac{1}{\theta}x_\gamma \exp(-\frac{x_\gamma^2}{2\theta^2}) + 2 F_N(x_\gamma) = \gamma$$
    Здесь $F_N(x)$ - функция распределения N(0,1)\\
    
    \subsection{Пример интерпритации распределения}
    [\textit{\textbf{17} - \href{https://studopedia.ru/15_37150_raspredelenie-maksvella.html}{Источник}}] Распределение Максвелла (Максвелла-Больцмана) лежит в основе кинетической теории газов, объясняющей многие фундаментальные свойства газов, включая давление и диффузию. Распределение Максвелла применимо к множеству свойств индивидуальных молекул в газе. О нём обычно думают как о распределении энергий молекул в газе, но оно может также применяться к распределению скоростей, импульсов, и модуля импульсов молекул. \\
    \textit{Ниже приведены некоторые примеры интепритации распределения Максвелла в виде функций плотности распределения:}
    \\
    \textbf{Распределение по вектору скорости:}
    [\textit{\textbf{18} - \href{https://ru.wikipedia.org/wiki/Распределение_Максвелла}{Источник}}]
    \begin{center}
        $\displaystyle p = mV \\
        \displaystyle f_{V}(v_{x}, v_{y}, v_{z}) = \sqrt{\Big(\frac{m}{2\pi kT}\Big)^{3}}\cdot exp\Big[\frac{-m(v_{x}^{2}, v_{y}^{2}, v_{z}^{2})}{2kT}\Big]$
    \end{center}
    \newpage
    \textbf{Распределение по проекции скорости:}
    \begin{center}
        $\displaystyle f_{м}(v_{i}) = \sqrt{\frac{m}{2\pi kT}}\cdot exp\Big[\frac{-m v_{i}^{2}}{2kT}\Big]$
    \end{center}
    \textbf{Распределение по модулю импульса:}
    \begin{center}
        $\displaystyle f_{p} = \int\limits_{\theta=0}^{\pi} \int\limits_{\phi=0}^{2\pi}f_{P}p^{2}sin(\theta)d\theta d\phi = 4\pi\sqrt{\Big(\frac{1}{2\pi mkT}\Big)^{3}}\cdot p^{2} \cdot exp\Big[\frac{-p^2}{2mkT}\Big]$
    \end{center}
    \textbf{Распределение по энергии:}
    \begin{center}
        $\displaystyle p^{2} = 2mE \textrm{ и } f_{E}dE = f_{p}dp \\
        \displaystyle f_{E} = f_{p}\frac{dp}{dE} = \frac{2\pi}{\sqrt{(\pi kT)^{3}}}\cdot\sqrt{E}\cdot exp\Big[\frac{-E}{kT}\Big]$
    \end{center}
    [\textit{\textbf{19} - \href{https://ru.wikipedia.org/wiki/Распределение_Максвелла}{Источник}}]
    А также распределение Максвелла можно записать как дискретное распределение по множеству состояний молекулы, нумеруемых символом $i$:
    \begin{center}
        $\displaystyle \frac{N_{i}}{N} = \frac{exp(-E_{i}/kT)}{\sum_{j}exp(-E_{j}/kT)}$
    \end{center}
    Через $E_{i}$ и $N_{i}$ обозначены энергия молекулы в $i$-м состоянии и число таких молекул соответственно, $T$ — температура системы, $N$ — общее число молекул в системе и $k$ — постоянная Больцмана.
    
    \subsection{Соотношения между распределениями}
    [\textit{\textbf{20} - \href{https://www.quora.com/What-is-a-good-explanation-of-the-Maxwell-Boltzmann-distribution}{Источник}}] Распределение Максвелла представляет собой величину 3-мерного вектора, компоненты которого независимы и нормально распределены со средним значением 0 и стандартным отклонением $a$. Если каждая случайная величина $\xi_{i}$ распределена как:
    \begin{center}
        $\displaystyle \xi \sim N(0, a^{2})$
    \end{center}
    Тогда:
    \begin{center}
        $\displaystyle Maxwell = \sqrt{\xi_{1}^{2} + \xi_{2}^{2} + \xi_{3}^{2}}$
    \end{center}
    Тогда распределение Максвелла можно связать с распределением $\chi^{2}$ с 3мя степенями свободы:
    \begin{center}
        $\displaystyle Maxwell = a\chi^{2}(3)$
    \end{center}
    
    
    \subsection{Описание способа моделирования выбранных случайных величин}
	[\textit{\textbf{21} - \href{https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxzZWVraW5ncWVkfGd4OjYyMmE4YmFkNWI0OWY5MzI}{Источник}}]
	Для моделирования случайных величин, распределенных по Максвеллу, будем использовать Алгоритм Джонка (Алгоритм Принятия-Отмены) - \textit{en: Johnk's Algorithm (Acceptance-Rejection Algorithm)}.\\
	\textbf{Описание алгоритма:} \\
	\textit{Входные данные: } $\xi_{1}, \xi_{2}, \xi_{3}, \xi_{4}$ - случайные числа из интервала $(0, 1)$ (каждую итерацию генерируется новая четвёрка случайных чисел) \\
	\textit{Выходные данные: } $\xi$
	\begin{enumerate}
        \item \textit{While} объём генерируемой выборки < $n$, где $n$ - требуемый объём выборки. 
        \begin{enumerate}
            \item $r = -ln\xi_{1}$ - случайное число из экспоненциального распределения.
	    \item $w_{1} = \xi_{2}^{2}, w_{2} = \xi_{3}^{2}$
	    \item If $w = w_{1} + w_{2} > 1$: возвращаемся на шаг \textbf{b} и начинаем новую итерацию цикла
	    \item $r = r - \frac{w_{1}}{w}ln\xi_{4}$
	    \item $\xi = \theta\sqrt{2r}$
        \end{enumerate}
	\end{enumerate}
    
    \textbf{\textit{График:}}
    \begin{center}
        \includegraphics[scale=1.2]{HW1/Maxwell/3_Model/Maxwell.png}
    \end{center}
    \newpage
    \textbf{\textit{Листинг кода:}}
    \lstinputlisting[language=Python]{HW1/Maxwell/3_Model/maxwell.py}
	
	
	\chapter{Основные понятия математической статистики}
	
	\section{Биномиальное распределение}
	\subsection{Генерация выборок}
	В данном разделе производится генерация выборок объёмов 5, 10, 100, 200, 400, 600, 800, 1000 биномиального распределения с параметрами $\displaystyle n = 87, \theta = 0.6$ \\
	
	\textbf{5} \\
	\begin{center}
	   \fbox{53 59 60 49 51} \\
	   \caption{Таблица 1.1: выборка из Биномиального распределения, n = 5}
	\end{center}
	\\
	\\
	
	\textbf{10} \\
	\begin{center}
	   \fbox{58 50 52 54 48 53 51 55 51 48} \\
	   \caption{Таблица 1.2: выборка из Биномиального распределения, n = 10}
	\end{center}
	\newpage
	
	\textbf{100} \\
	\begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_100.png}}
        \caption{Таблица 1.3: выборка из Биномиального распределения, n = 100}
        \label{fig:my_label}
    \end{figure}

	\textbf{200} \\
	\begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_200.png}}
        \caption{Таблица 1.4: выборка из Биномиального распределения, n = 200}
        \label{fig:my_label}
    \end{figure}

	\newpage
	\textbf{400} \\
	\begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_400.png}}
        \caption{Таблица 1.5: выборка из Биномиального распределения, n = 400}
        \label{fig:my_label}
    \end{figure}

	\newpage
	\textbf{600} \\
	\begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_601.png}}
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_602.png}}
        \caption{Таблица 1.6: выборка из Биномиального распределения, n = 600}
        \label{fig:my_label}
    \end{figure}

	\newpage
	\textbf{800} \\
	\begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_801.png}}
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_802.png}}
        \caption{Таблица 1.7: выборка из Биномиального распределения, n = 800}
        \label{fig:my_label}
    \end{figure}

	\newpage
	\textbf{1000} \\
	\begin{figure}[h]
        \centering
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_1001.png}}
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_1002.png}}
        \fbox{\includegraphics[scale=0.7]{HW2/Binomial/1_Selections/Binomial_1003.png}}
        \caption{Таблица 1.8: выборка из Биномиального распределения, n = 1000}
        \label{fig:my_label}
    \end{figure}
	\\
	\newpage
	\subsection{Построение эмпирической функции распределения}
	Из \textbf{\textit{Определения 1.11}}[\textit{\textbf{22} - Фомин Д. Б, Чухно А. Б., "Математическая статистика. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 7}}]
	можем видеть, что:\\
	Пусть $X$ - некоторая выборка\\
	$\displaystyle\mu_{n}(y) = \sum_{i=1}^{n}Ind(X_{i} \leq y), y \in \mathbb{R}$ - случайная величина, равная числу элементов выборки $X$ меньших или равных $y$. Тогда функцию:
	\begin{center}
	    $\displaystyle \widehat{F}_{n}(y) = \frac{\mu_{n}(y)}{n}$
	\end{center}
	Будем называть Эмпирической Функцией Распределения, соответствующей выборке $X$. \\
	Эмпирическая кумулятивная функция распределения, возвращающая на основе выборки и числа t долю значений в выборке, меньших t, представлена в листинге ниже (под именем \textit{CDF}). \\
	\textbf{\textit{Листинг кода:}}
    \lstinputlisting[language=Python]{HW2/Binomial/2_EmpFuncs/EmpBin.py}

    Ниже представлены графики Эмпирической Функции Распределения для каждой выборки с графиками функции распределения случайной величины:\\
    Также, проанилизировав приведённые графики, можно сделать вывод, что при увеличении объёма выборки график кумулятивной эмпирической функции распределения всё больше "стремится" к графику функции распределения случайной величины. Также это подтверждается \textbf{\textit{Теоремой}} [\textit{\textbf{23} - Фомин Д. Б, Чухно А. Б., "Математическая статистика. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 8}}], гласящей о том, что:\\
    \begin{center}
        $\displaystyle \textrm{Для } \forall x \in \mathbb{R} \textrm{ и для } \forall \epsilon>0 \textrm{ при } n \longrightarrow \infty \\
        \displaystyle P\Big(\Big|\widehat{F}_{n}(x) - F(x)\Big| < \epsilon\Big) \longrightarrow 1$
    \end{center}
    Другими словами: для произвольного фиксированного $y \in \mathbb{R}$ э.ф.р. $\widehat{F}_{n}(y)$ с увеличением объема выборки $n$ стремится к значению функции распределения $F(y)$.

    \begin{center}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/5.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/10.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/100.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/200.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/400.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/600.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/800.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_EmpFuncs/1000.png}}
    \end{center}

    Для посчета $D_{n, m}$ необходимо описать функцию супремума, которая описана ниже:
    \lstinputlisting[language=Python]{HW2/Binomial/2_Dnm/BinSup.py}
    
    Описав функцию супремума, опишем и саму функцию подсчета $D_{n, m}$: 
    \lstinputlisting[language=Python]{HW2/Binomial/2_Dnm/Dnm.py}
    
    Произведём расчёты функции $D_{n,m}$ для каждой пары выборок с помощью скрипта на языке Python. Получим матрицу значений, которая, что примечательно, симметрична относительно главной диагонали.
    
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/2_Dnm/BinDnm.png}}
    \end{center}

  \subsection{Построение гистограммы частот}
  Ниже представлены гистограммы частот для каждой выборки с графиками функции вероятности: \\
  Из приведенных ниже гистограмм частот можно сделать вывод, что с увеличением объёма выборки - её гистограмма частот "стремится" к функции вероятности. Это иллюстрирует следующую теорему:\\
      
      [\textit{\textbf{24} \href{https://studfile.net/preview/5172427/}{Источник}}] Эти графики иллюстрируют теорему Бернулли, которая в свою очередь является частным случаем теоремы Чебышева. Теорема Бернулли констатирует, что частота $m(A)$ наблюдения случайного события $A$ стремится в аналогичных условиях к вероятности его возникновения, т.е.
        $$ \forall \epsilon >0:\lim_{n\rightarrow\infty}P\left(\Big|\frac{m(A)}{n}-P(A)\Big|<\epsilon\right)=1$$
  \begin{center}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/1.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/10.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/100.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/200.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/400.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/600.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/800.png}}
        \fbox{\includegraphics[scale=1]{HW2/Binomial/3_HISTS/1000.png}}
    \end{center}

    \textbf{Листинг кода:} \\
    \lstinputlisting[language=Python]{HW2/Binomial/3_HISTS/BinHist.py}
    
    \newpage
    \subsection{Вычисление выборочных моментов}
    Оценка математического ожидания является \textbf{несмещенной} и \textbf{состоятельной}\\
    Оценка дисперсии является \textbf{смещенной} и \textbf{состоятельной}\\
    Докажем эти факты: \\
    \textbf{Доказательство несмещённости оценки математического ожидания} \\
    По \textit{\textbf{Определению 2.6}} [\textit{\textbf{25} - Фомин Д. Б, Чухно А. Б., "Математическая статистика. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 12}}] знаем, что, если смещение $b(\theta)$ равно нулю, то оценка является \textbf{несмещённой}, где
    \begin{center}
        $\displaystyle b(\theta) = M_{\theta}T(X) - \tau(\theta)$
    \end{center}
    Таким образом, в случае выборочного среднего, имеем:
    \begin{center}
        $\displaystyle \tau(\theta) = MX_{i}, i = 1,\cdots,n \\
        \displaystyle \overline{X} = T(X) = \frac{1}{n}\sum_{i=1}^{n}X_{i}, n$ - объём выборки. 
    \end{center}
    Найдем математическое ожидание выборочного среднего: \\
    $\displaystyle MT(X) = \frac{1}{n}M\sum_{i=1}^{n}X_{i} = \frac{1}{n}\sum_{i=1}^{n}M X_{i} = \frac{1}{n}nMX_{i} = MX_{i}$ \\
    Из приведенного доказательства видно, что \\
    $\displaystyle b(\theta) = M_{\theta}T(X) - \tau(\theta) = MX_{i} - MX_{i} = 0$\\
    Следовательно, оценка является \textbf{несмещённой}.\\
    
    \textbf{Доказательство состоятельности оценки математического ожидания}\\
    Воспользуемся утверждением [\textit{\textbf{26} - Фомин Д. Б, Чухно А. Б., "Математическая статистика. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 15}}] о том, что для проверки состоятельности несмещенной оценки достаточно убедиться в том, что ее дисперсия стремится к 0 при n, стремящемуся к $\infty$.\\
    Также воспользуемся \textit{\textbf{Свойством 4}} [\textit{\textbf{27} - Фомин Д. Б, Чухно А. Б., "Теория Вероятности. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 72}}], которое гласит, что:
    \begin{center}
        $\displaystyle D(c \cdot \xi) = x^{2} \cdot D\xi$
    \end{center}
    Таким образом: \\
    $\displaystyle DT(X) = D\Big(\frac{1}{n} \cdot \sum_{i=1}^{n}X_{i}\Big) = \frac{1}{n^2}\Big(\sum_{i=1}^{n}D X_{i}\Big) = \frac{1}{n^2}nDX_{i} = \frac{1}{n}DX_{i}$\\
    Тогда: \\
    $\displaystyle \lim_{n \longrightarrow \infty}DT(X) = \lim_{n \longrightarrow \infty}\frac{1}{n}DX_{i} = 0$ \\
    т.к. $\displaystyle\frac{1}{n} \longrightarrow 0 \textrm{ при } n \longrightarrow \infty$\\
    Тогда, можем сделать вывод, что оценка выборочного среднего \textbf{несмещённая} и \textbf{состоятельная}.\\
    
    \textbf{Доказательство смещенности оценки дисперсии}\\
    Воспользуемся тем же утверждением, что и в доказательстве несмещенности оценки выборочного среднего. А также воспользуемся утверждениями с уже упомянтой \textbf{\textit{стр. 12}} лекций курса Математической статистики. Тогда:\\
    \begin{center}
        $\displaystyle \textrm{Пусть } Y_{i} = X_{i} - a_{1}, a_{1} = M\xi (1)\\
        \displaystyle \textrm{Следовательно: } \overline{Y} = \overline{X} - M\overline{X} (2) \\
        $
    \end{center}
    Тогда: \\
    $\displaystyle MT(X) = \frac{1}{n}M\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2} =$ по определению: $\overline{Y} = \sum_{j = 1}^{n}Y_{j}$, поэтому $\displaystyle =\\= \frac{1}{n}M\sum_{i=1}^{n}(Y_{i}^{2} + \overline{Y}^{2} - 2\Big(\frac{1}{n}\sum_{j = 1}^{n}Y_{j}\Big)Y_{i}) = \frac{1}{n}M\Big(\sum_{i=1}^{n}Y_{i}^{2} + \sum_{i=1}^{n}\overline{Y}^{2} - 2\Big(\frac{1}{n}\sum_{i, j = 1}^{n}Y_{j}Y_{i}\Big)\Big) =\\= \frac{1}{n}\Big(\sum_{i=1}^{n}MY_{i}^{2} + \sum_{i=1}^{n}M\overline{Y}^{2} - \frac{2}{n}\sum_{i, j = 1}^{n}M(Y_{j}Y_{i})\Big) = $ 
    \\
    \begin{tcolorbox}
        \textbf{Установим следующие факты:}\\
        \textbf{\textit{1}}. $\displaystyle MY_{i} =$ опираясь на утверждение (1) $= MX_{i} - M(MX_{i}) = $ Мат ожидание - константа. Мат ожидание константы равна этой же константе, следовательно $= MX_{i} - MX_{i} = 0$\\
        \textbf{\textit{2}}. $\displaystyle MY_{i}^{2} =$ вычтем из Мат. Ожидания нулевую величину $=\\= MY_{i}^{2} - (MY_{i})^{2} = DY_{i}$\\
        \textbf{\textit{3}}. Если $i \neq j$, то верно следующее:\\
        $\displaystyle M(Y_{i}Y_{j}) = MY_{i}MY_{j}$\\
        Учитывая пункт \textbf{1} ($MY_{i} = 0$), получим:\\
        $\displaystyle M(Y_{i}Y_{j}) = MY_{i}MY_{j} = 0$\\
        \end{tcolorbox}
        \begin{tcolorbox}
        \textbf{\textit{4}}. $\displaystyle M\overline{Y}^{2} = \frac{1}{n^2}\sum_{i, j = 1}^{n}M(Y_{i}Y_{j}) = $ имеем сумму сумм $= \displaystyle\frac{1}{n^2}\Big(\sum_{i \neq j}M(Y_{i}Y_{j}) +\\+ \sum_{i = 1}^{n}MY_{i}^{2}\Big) = $ опираясь на пункт \textbf{3}, значем, что $M(Y_{i}Y_{j}) = 0$, тогда $= \frac{1}{n^2}\sum_{i = 1}^{n}MY_{i}^{2} = $ опираясь на пункт \textbf{2}, знаем, что $MY_{i}^{2} = DY_{i}$, тогда $= \displaystyle\frac{1}{n^2}\sum_{i=1}^{n}DY_{i} = \frac{1}{n^2}nDY_{i}= \frac{1}{n}DY_{i}$
        \end{tcolorbox}
    $\displaystyle = \frac{1}{n}\Big(\Big(1 - \frac{2}{n}\Big)\sum_{i = 1}^{n}MY_{i}^{2} + \sum_{i = 1}^{n}M\overline{Y}^{2}\Big) = \frac{1}{n}\Big((1 - \frac{2}{n}\Big)nDY_{i} + DY_{i}\Big) =\\= \frac{1}{n}\Big(DY_{i}\Big(n + 1 - 2\Big)\Big) = \frac{n - 1}{n}DY_{i} = \frac{n - 1}{n}D(X_{i} - MX_{i}) = $ Опираясь на то, что $\displaystyle D(\xi + c) = D\xi$, получаем $\displaystyle = \frac{n - 1}{n}DX_{i}$
    \\
    Таким образом, $\displaystyle b(\theta) = DX_{i} - \frac{n - 1}{n}DX_{i} \neq 0$\\
    Следовательно, оценка дисперсии \textbf{смещённая}.\\
    
    \textbf{Доказательство состоятельности оценки дисперсии}\\
    По определению: $\displaystyle \overline{S}^{2} = \frac{1}{n}\sum_{i = 1}^{n}(X_{i} - \overline{X})^{2} = \frac{1}{n}\sum_{i = 1}^{n}(X_{i}^{2} + (\overline{X})^{2} - 2X_{i}\overline{X}) = \frac{1}{n}\sum_{i=1}^{n}X_{i}^{2} -\\- \frac{2}{n}\overline{X}\sum_{i=1}^{n}X_{i} + \frac{1}{n}(\overline{X})^{2}\sum_{i=1}^{n} = \overline{X^{2}} - 2\overline{X}\cdot\overline{X} + (\overline{X})^{2} = \overline{X^{2}} - (\overline{X})^{2}$\\
    Статистика $\overline{X}$ представляет из себя сумму независимых одинаково распределенных случайных величин, для которых определено математическое ожидание $M|X_{1}| < \infty$ \\
    Опишем \textit{Закон Больших Чисел в форме Хинчина} [\textit{\textbf{28} - \href{http://angtu.ru/universitet/kafedry-angtu/math/posobiya/metTV_ch_2.pdf}{Источник, \textbf{стр. 77}}}]: \\
    Пусть на одном пространстве элементарных событий задана последовательность независимых одинаково распределенных случайных величин $X_{1}, x_{2}, \cdots, X_{n}$, для которых определено математическое ожидание. Тогда:
    \begin{center}
        $\displaystyle \lim_{n \longrightarrow \infty} P\Bigg(\Bigg|\frac{X_{1}+\cdots+X_{n}}{n} - M(X_{1})\Bigg| < \epsilon \Bigg) = 1, \textrm{  } \forall \epsilon > 0$
    \end{center}
    Тогда, опираясь на вышеописанные записи и Закон Больших Чисел в форме Хинчина, получим, что:\\
    $\displaystyle \overline{X} \longrightarrow_{n \longrightarrow \infty}^{p} MX_{i}$ \\
    Тогда:\\
    $\displaystyle\overline{X}^{2} - (\overline{X})^{2} \longrightarrow_{n \longrightarrow \infty}^{p} MX_{i}^{2} - (MX_{i})^{2} = DX_{i}$\\
    Таким образом, оценка дисперсии \textbf{состоятельна}.\\
    
    Истинное выборочное среднее при параметрах $\displaystyle n = 87, \theta = 0.6$:\\
    $\displaystyle n \cdot \theta = 87 \cdot 0.6 = 52.2$\\
    Истинное выборочная дисперсия при параметрах $\displaystyle n = 87, \theta = 0.6:$\\
    $\displaystyle n \cdot \theta \cdot (1 - \theta) = 87 \cdot 0.6 \cdot 0.4 = 20.88$
    \\
    \begin{center}
        \includegraphics[scale = 1.2]{HW2/Binomial/4_1/1.png}
    \end{center}
    Вычислим погрешность выборочного среднего и выборочной дисперсии, зная истинные значения данных величин:
    \begin{center}
        \includegraphics[scale = 1.2]{HW2/Binomial/4_1/2.png}
    \end{center}
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW2/Binomial/4_1/BinMoments.py}
    
    \section{Распределение Максвелла}
    \subsection{Генерация выборок}
    В данном разделе производится генерация выборок объёмов 5, 10, 100, 200, 400, 600, 800, 1000 распределения Максвелла с параметром $\theta = 3$\\
    \textbf{5}\\
    \begin{center}
        \fbox{5.978659 3.882685 5.332642 3.792912 2.248517}\\
        Выборка из распределения Максвелла, n = 5
    \end{center}
    
    \textbf{10}\\
    \begin{center}
        \fbox{2.437915 2.313844 4.983702 5.628353 3.314616 4.39135 2.641108 4.972171 4.316611 5.1504}\\
        Выборка из распределения Максвелла, n = 10
    \end{center}
    
    \newpage
    \textbf{100}\\
    \begin{center}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/100.png}}
        Выборка из распределения Максвелла, n = 100
    \end{center}
    
    \textbf{200}\\
    \begin{center}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/200.png}}
        Выборка из распределения Максвелла, n = 200
    \end{center}
    
    \newpage
    \textbf{400}\\
    \begin{center}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/400.png}}
        Выборка из распределения Максвелла, n = 400
    \end{center}
    
    \newpage
    \textbf{600}\\
    \begin{center}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/601.png}}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/602.png}}
        Выборка из распределения Максвелла, n = 600
    \end{center}
    
    \newpage
    \textbf{800}\\
    \begin{center}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/801.png}}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/802.png}}
        Выборка из распределения Максвелла, n = 800
    \end{center}
    
    \newpage
    \textbf{1000}\\
    \begin{center}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/1001.png}}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/1002.png}}
        \fbox{\includegraphics[scale = 1]{HW2/Maxwell/1_Selections/1003.png}}
        Выборка из распределения Максвелла, n = 1000
        
    \end{center}
    
    \subsection{Построение эмпирической функции распределения}
    Опираясь на утверждения об Эмпирической Функции Распределения из пункта \textbf{2.1.2}, построим графики ЭФР.
    Эмпирическая кумулятивная функция распределения, возвращающая на основе выборки и числа t долю значений в выборке, меньших t, представлена в листинге ниже (под именем \textit{CDF}). \\
	\textbf{\textit{Листинг кода:}}
    \lstinputlisting[language=Python]{HW2/Maxwell/2_EmpFuncs/EmpMax.py}
    Ниже представлены графики Эмпирической Функции Распределения для каждой выборки с графиками функции распределения случайной величины:\\
    Также, проанилизировав приведённые графики, можно сделать вывод, что при увеличении объёма выборки график кумулятивной эмпирической функции распределения всё больше "стремится" к графику функции распределения случайной величины. Также это подтверждается \textbf{\textit{Теоремой}} [\textit{\textbf{29} - Фомин Д. Б, Чухно А. Б., "Математическая статистика. Курс лекций для студентов кафедры компьютерной безопасности": \textbf{стр. 8}}], гласящей о том, что:\\
    \begin{center}
        $\displaystyle \textrm{Для } \forall x \in \mathbb{R} \textrm{ и для } \forall \epsilon>0 \textrm{ при } n \longrightarrow \infty \\
        \displaystyle P\Big(\Big|\widehat{F}_{n}(x) - F(x)\Big| < \epsilon\Big) \longrightarrow 1$
    \end{center}
    Другими словами: для произвольного фиксированного $y \in \mathbb{R}$ э.ф.р. $\widehat{F}_{n}(y)$ с увеличением объема выборки $n$ стремится к значению функции распределения $F(y)$.
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/5.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/10.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/100.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/200.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/400.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/600.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/800.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_EmpFuncs/1000.png}}
    \end{center}
    
    Подсчёт функции $D_{n, m}$ производится по алгоритму, аналогичному алгоритму из пункта \textbf{2.1.2}
    
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/2_Dnm/Dnm.png}}
    \end{center}
    
    \subsection{Построение гистограммы частот}
     Ниже представлены гистограммы частот для каждой выборки с графиками функции плотности распределения: \\
      Из приведенных ниже гистограмм частот можно сделать вывод, что с увеличением объёма выборки - её гистограмма частот "стремится" к функции плотности распределения. Это иллюстрирует следующую теорему:\\
      
      [\textit{\textbf{30} - Н. И. Чернова, "Лекции по математической статистике", Нижегородский Государственный Университет - \textbf{стр. 12, стр. 20}}] Пусть распределение $F$ абсолютно непрерывно, $f$ - его истинная плотность. Пусть, кроме того, число $k$ интервалов группировки не зависит от $n$. Тогда справедлива Теорема:\\
      \begin{center}
          При $\displaystyle n \longrightarrow \infty \textrm{  } \forall j = 1,\cdots,k \\
          \displaystyle l_{j}\cdot f_{j} = \frac{v{i}}{n} \longrightarrow^{p} P(X_{1} \in A_{j}) = \int_{A_{j}}f(\chi)d\chi$
      \end{center}
    Предполагаемую область значений случайной величины $\xi$ делят независимо от выборки на некоторое количество интервалов (не обязательно одинаковых). Пусть $A_{1},\cdots, A_{k}$ - интервалы на прямой, называемые интервалами группировки. Обозначим для $j = 1,\cdots,k$ через $v_{j}$ число элементов выборки, попавших в интервал $A_{j}$. $l_{j}$ - длина интервала $A_{j}$\\
    Данная теорема утверждает, что площадь столбца гистограммы, построенного над интервалом группировки, с ростом объема выборки сближается с площадью области под графиком плотности над этим же интервалом.
  \begin{center}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/5.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/10.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/100.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/200.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/400.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/600.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/800.png}}
        \fbox{\includegraphics[scale=1]{HW2/Maxwell/3_HISTS/1000.png}}
    \end{center}

    \textbf{Листинг кода:} \\
    \lstinputlisting[language=Python]{HW2/Maxwell/3_HISTS/MaxHist.py}
    
    \newpage
    \subsection{Вычисление выборочных моментов}
    Оценка математического ожидания является несмещенной и состоятельной (факты, доказанные в пункте \textbf{2.1.4})\\
    Оценка дисперсии является смещенной и состоятельной (факты, доказанные в пункте \textbf{2.1.4})\\
    
    Истинное выборочное среднее при параметрах $\displaystyle \theta = 3$:\\
    $\displaystyle 2 \cdot \theta \cdot \sqrt{\frac{2}{\pi}} = 2 \cdot 3 \cdot \sqrt{\frac{2}{\pi}} = 4.787$\\
    Истинное выборочная дисперсия при параметрах $\displaystyle \theta = 3:$\\
    $\displaystyle \theta^{2} \cdot \Big(3 - \frac{8}{\pi} \Big) = 9 \cdot \Big(3 - \frac{8}{\pi} \Big) = 4.081$
    \\
    \begin{center}
        \includegraphics[scale = 1.2]{HW2/Maxwell/4_1/1.png}
    \end{center}
    Вычислим погрешность выборочного среднего и выборочной дисперсии, зная истинные значения данных величин:
    \begin{center}
        \includegraphics[scale = 1.2]{HW2/Maxwell/4_1/2.png}
    \end{center}
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW2/Maxwell/4_1/MaxMoments.py}

    \chapter{Построение точечных оценок параметра распределения}
    \section{Биномиальное распределение}
    \subsection{Получение оценок методом моментов и методом максимального правдоподобия}
    \subsubsection{Метод Моментов}
    [\textit{\textbf{31} - А. Б. Чухно, Д. Б. Фомин, "Математическая Статистика. Курс лекций
    для студентов кафедры компьютерной
    безопасности " - \textbf{стр. 15}}] Пусть имеется выборка X = $(X_1, \dots, X_n)$ из распределения $\mathfrak{L}(\xi), \mathfrak{L}(\xi)\in \mathcal{F} = {F_\theta, \theta\in\Theta}$, где $\theta = (\theta_1, \dots, \theta_r) \in \mathbb{R}^r$. Пусть у случайной величины $\xi$ имеются первые r моментов, т.е. $\alpha_k = M_\theta(\xi^k) < \infty$, являющиеся функциями от неизвестного $\theta: \alpha_k = \alpha_k(\theta), k = \overline{1,r}$.\\
    Рассмотрим систему:
    $$\Bigg \{ \alpha_k(\theta) = \displaystyle \widehat{\alpha}_k, k = \overline{1,r}$$
    в которой r неизвестных $\theta_1, \dots, \theta_r$. Эта система однозначно разрешима и ее решением являются $\widehat{\theta}_i = \phi_i(\widehat{\alpha}_1, \dots, \widehat{\alpha}_k), \phi_i - $ некоторая функция.\\
    Оценки $\widehat{\theta}_i$ будем называть оценками, построенными по методу моментов. Заметим, что есть функция $\phi_i$ является непрерывной, то оценка $\widehat{\theta}_i$ является состоятельной.\\
    На практике для получения оценки параметра распределения метом моментов необходимо приравнять теоретические моменты соответствующим эмпирическим моментам того же порядка.\\

    Для случайной величины $\xi$, распределенной по биномиальному закону с параметрами $n$ и $\theta$ известны следуюшие равенства:\\
    $\displaystyle M\xi = n\theta$\\
    $\displaystyle D\xi = n\theta(1 - \theta)$\\
    
    А также:\\
    $n\theta = \displaystyle \overline{X} = \frac{1}{n}\sum_{i=1}^{n}X_{i}$ (т.к. $M\xi = \overline{X}$)\\
    $n\theta(1-\theta) = \displaystyle \overline{S}^{2} = \frac{1}{n}\sum_{i=1}^{n}\Big(X_{i}-\overline{X}\Big)^{2}$ (т.к. $D\xi = \overline{S}^{2}$)\\
    $\displaystyle 1-\theta = \frac{\overline{S}^{2}}{\overline{X}}$\\
    $\displaystyle \hat{\theta} = 1 - \frac{\overline{S}^{2}}{\overline{X}}$\\
    Написав скрипт на ЯП Python, получаем для каждой выборки, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW3/Binomial/BinMom/BinMom.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW3/Binomial/BinMom/BinMom.py}
    
    \subsubsection{Метод Максимального Правдоподобия}
    [\textit{\textbf{32} - А. Б. Чухно, Д. Б. Фомин, "Математическая Статистика. Курс лекций
    для студентов кафедры компьютерной
    безопасности " - \textbf{стр. 16}}] Пусть есть выборка $X = (X_{1}, \cdots, X_{n})$ из распределения $\manthal{L}(\xi)$. Пусть $f_{\theta}(x)$ есть функция плотности случайной величины $\xi$, которая известна с точностью до параметра из распределения (в дискретном случае вместо функции плотности берем функцию вероятности $P_{\theta}(\xi = x)$). Пусть $\overline{x} = (x_{1},\cdots,\x_{n})$)\\
    \textbf{Определение 3.2} Функцию, заданную равенством $\displaystyle L(\overline{x}, \theta) = \prod_{i=1}^{n}f_{\theta}(\xi)$ будем называть функцией правдоподобия.\\

    \textbf{Определение 3.3} Оценкой максимального правдоподобия называется построеннная по реализации выборки $\overline{x}$ значение $\widehat{\theta} = \underset{\theta\in\Theta }{arg\text{ }max}\textit{L}(\overline{x};\theta)$\\

    То есть, чтобы найти оценку максимального правдоподобия (о.м.п.), надо найти такое значение $\theta$, при котором функция правдоподобия принимает
    максимальное значение.\\

    Если для каждого $\overline{x}$ из выборочного пространства X максимум $\textit{L}(\overline{x}; \theta)$ достигается в некоторой внутренней точке и $\textit{L}(\overline{x}; \theta)$ дифференцируема по $\theta$, то $\widehat{\theta}(\oveline{x})$ удовлетворяет условию\\
    \begin{center}
        $\displaystyle \frac{d L(\overline{x}; \theta)}{d \theta} = 0$
    \end{center}
    Вместо функции правдоподобия для простоты часто рассматривают следующую функцию\\
    \begin{center}
        $ln L(\overline{x}; \theta) = \sum_{i=1}^{n}ln f_{\theta}(x_{i})$
    \end{center}\\

    Функцией максимального правдоподобия для биномиального распределения будет функция:\\
    $ \displaystyle L(X,\theta,n)=\prod_{i=1}^{N}P\{X_1=x_1\}\cdot\ldots\cdot P\{X_N=x_N\}=C_n^{x_1}\cdot\ldots\cdot C_n^{x_N}\cdot 
    \theta^{x_1+\ldots+x_N}(1-\theta)^{nN-(x_1+\ldots+x_N)}
    $\\
    где $x_i$ - наблюдаемые значения в выборке, $N$ - объём выборки, $P\{X_i=x_i\}=C_{n}^{x_i} \theta^{x_i} (1-\theta)^{n-x_i}$. Её логарифмическая версия имеет вид\\
    $\displaystyle ln(L(X,\theta,n))=\sum_{i=1}^N \ln(P\{X_i=x_i\})=
    \sum_{i=1}^N \left(\ln(C_{n}^{x_i})\right) +(x_1+\ldots+x_N)\ln(\theta)+(nN-(x_1+\ldots+x_N))\ln(1-\theta)$
    Чтобы найти оценки, необходимо приравнять производные этой функции по $n$ и по $\theta$ к $0$ и решить полученную систему уравнений. 
    К сожалению, в случае неизвестного $n$ система неразрешима аналитически. Однако при известном $n$ можно получить выражение для $\theta$:$ \displaystyle L'_{\theta}(X,\theta,n)=\frac{N\bar{x}}{\theta}-\frac{n-N\bar{x}}{1-\theta}=0=>\hat{\theta}=\frac{\bar{x}}{n} = \frac{\sum_{i=1}^{N}X_{i}}{n \cdot N}$\\

    Написав скрипт на ЯП Python, получаем для каждой выборки, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW3/Binomial/BinMax/BinMax.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW3/Binomial/BinMax/BinMax.py}
    
    
    \subsection{Поиск оптимальных оценок}
    Перед определением оптимальной оценки необходимо описать следующие понятия:\\
    Эффективная оценка – несмещенная оценка, дисперсия которой совпадает с нижней гранью в неравенстве Крамера-Рао. Эффективная оценка для  $\tau(\theta)$  есть несмещенная оценка с минимальной дисперсией для  $\tau(\theta)$ .\\
    \textbf{Эффективная оценка является оптимальной.}\\
    Теорема: Для того чтобы несмещенная оценка $T = T(X)$ для $\tau(\theta)$ была
    эффективной, необходимо и достаточно, чтобы
    $$\frac{\partial \ln L}{\partial \theta} = A(\theta)(T(X)-\tau(\theta))$$
    где $A(\theta)$ – функция, зависящая только от $\theta,\  T(X)$ – несмещенная оценка
    для $\tau(\theta)$, если $M_{\theta}T\Big(X_{1},\cdots, X_{n}\Big) = \tau(\theta)$ для всех $\theta$. При этом 
    $$Dt=\Bigg|\frac{\tau'(\theta)}{A(\theta)}\Bigg|$$
    Преобразуем логарифмическую функцию правдоподобия
    $$\frac{\partial l(\theta|X)}{\partial \theta} = \frac{1}{\theta}\sum_{i=1}^n x_i - \frac{1}{1-\theta}\Bigg( nN-\sum_{i=1}^nx_i\Bigg)$$
    $$\frac{\partial l(\theta|X)}{\partial \theta} = \frac{nN}{\theta(1-\theta)}\Bigg(\frac{\sum_{i=1}^nx_i}{nN}-\theta\Bigg)$$
    $$\frac{\partial l(\theta|X)}{\partial \theta} =\frac{nN}{\theta(1-\theta)}\Bigg(\frac{\overline{X}}{n}-\theta\Bigg)$$
    при $\tau(\theta) = \theta; A =\frac{nN}{\theta(1 - \theta)}; T = \frac{\overline{X}}{n}$
    получаем 
    $$\frac{\partial \ln L}{\partial\theta}= A(\theta)(t-\tau(\theta))$$
    Следовательно $\frac{\overline{X}}{n}$
    – эффективная оценка для $\tau(\theta) = \theta$. Так как
    эффективная оценка является оптимальной, мы получили оптимальную
    оценку.\\

    Написав скрипт на ЯП Python, получаем для каждой выборки, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW3/Binomial/BinOpt/BinOpt.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW3/Binomial/BinOpt/BinOpt.py}
    

    \section{Распределение Максвелла}
    \subsection{Получение оценок методом моментов и методом максимального правдоподобия}
    \subsubsection{Метод Моментов}
    Для случайной величины $\xi$, распределенной по распределению Максвелла, имеем:\\
    $\displaystyle M\xi = 2\theta\sqrt{\frac{2}{\pi}}$\\
    Тогда:\\
    $\displaystyle \theta = \frac{M\xi \sqrt{\pi}}{2\sqrt{2}}$\\
    Перейдем от мат. ожидания к его оценке ($\overline{X}$) и получим:\\
    $\displaystyle \hat{\theta} = \frac{\overline{X}\sqrt{\pi}}{2\sqrt{2}}$\\

    Написав скрипт на ЯП Python, получаем для каждой выборки, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW3/Maxwell/MaxMom/Maxmom.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW3/Maxwell/MaxMom/Maxmom.py}\\

    Если же для оценки параметра использовать выборочную дисперсию, то получим, что\\

    $\displaystyle D\xi = 3\theta^{2} - \frac{8\theta^{2}}{\pi}\\
    \overline{\theta} = \sqrt{\frac{\overline{S}^{2}}{3 - \frac{8}{\pi}}}$

    Написав скрипт на ЯП Python, получаем для каждой выборки, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW3/Maxwell/MaxMom/Maxmom2.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW3/Maxwell/MaxMom/Maxmom2.py}\\

    \subsubsection{Метод Максимального Правдоподобия}
    Функцией максимального правдоподобия для распределения Максвелла будет функция:
    $$ L(X,\theta)=\prod_{i=1}^{N}f(x_1)\cdot\ldots\cdot f(x_N)=\prod_{i=1}^{N} \sqrt{\frac{2}{\pi}}\frac{x_i^2}{\theta^3} e^{-\frac{x_i^2}{2\theta^2}}
    $$
    где $x_i$ - наблюдаемые значения в выборке, $N$ - объём выборки. Её логарифмическая версия имеет вид
    $$ ln(L(X,\theta))=\sum_{i=1}^N \ln(f(x_i))=\sum_{i=1}^N \left(\ln(\sqrt{{2}{\pi}})+\ln(x_i^2)-3\ln(\theta)-\frac{x_i^2}{2\theta^2}\right)$$
    Чтобы найти оценку, необходимо приравнять производную этой функции по $\theta$ к $0$ и решить полученное уравнение.\\
    $ \displaystyle L'_{\theta}=\sum_{i=1}^{N}\left( -\frac{3}{\theta}+\frac{x_i^2}{\theta^3}\right)=0=>\sum_{i=1}^{N}\Big(-\frac{3}{\theta}\Big) + \sum_{i=1}^{N}\frac{x_{i}^{2}}{\theta^{3}} = 0 \\
    => -\frac{3N}{\theta} + \sum_{i=1}^{N}\frac{x_{i}^{2}}{\theta^{3}} = 0 => -\frac{3N}{\theta} + \frac{1}{\theta^{3}}\sum_{i=1}^{N}x_{i}^{2} = 0\\
    => -\frac{3N}{\theta} + \frac{N}{\theta^{3}}\overline{x}^{2} = 0 => 3 = \frac{\overline{x}^{2}}{\theta^{2}} \\
    => \hat{\theta} = \sqrt{\frac{\sum_{i=1}^{N}x_{i}^{2}}{3}}$

    Написав скрипт на ЯП Python, получаем для каждой выборки, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW3/Maxwell/MaxMax/MaxMax.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW3/Maxwell/MaxMax/MaxMax.py}\\
    
    \subsection{Поиск оптимальных оценок}
    Для поиска оптимальной оценки воспользуемся тем же методом, что и
    для дискретного распределения. 
    Преобразуем логарифмическую функцию правдоподобия
    $$ \frac{\partial\ln(\theta|X)}{\partial\theta} = \frac{-3n}{\theta} + \frac{\sum_{i=1}^n x_i^2}{\theta^3} =\frac{3n}{\theta^3}\Bigg(\frac{\sum_{i=1}^n x_i^2}{3n}-\theta^2\Bigg)$$
    при $\tau(\theta) = \theta^2 ; \ A = \frac{3n}{\theta^3}; \ t =\frac{\sum_{i=1}^n x_i^2}{3n}$
    , получаем
    $$\frac{\partial \ln L}{\partial \theta} = A(\theta)(t - \tau(\theta)) $$
    Следовательно $\displaystyle \frac{\sum_{i=1}^n x_i^2}{3n}$
    – эффективная оценка для $\tau(\theta) =\theta^2$
    . Так как
    эффективная оценка является оптимальной, мы получили оптимальную
    оценку.\\
    Так как $\tau(\theta)$ не равна $\theta$, построим оптимальную оценку для $\theta$\\
    Воспользуемся следующими теоремами, необходимыми для дальнейшего хода решения:\\
    \textbf{Теорема Рао-Блекуэлла-Колмогорова:}\\
    Оптимальная оценка, если существует, является функцией от достаточной
    статистики.\\
    \textbf{Теорема о полноте семейства экспоненциальных функций}\\
    Пусть $F = F_{\theta}$, $\theta \in \Theta$ - экспоненциальное семейство и ф-ия $A(\theta)$ и параметрическое пространство \Theta таковы, что $A(\theta)$ содержит некоторый отрезок, когда \theta пробегает множество \Theta. Тогда $\displaystyle T(X) = \sum_{i=1}^{n}B(X_{i})$ является полной и достаточной статистикой.\\
    А также воспользуемся \textbf{теоремой Лемана-Шеффе}\\
    Если Y – полная и достаточная статистика, $\phi: M_{\phi}(Y) = \theta$, тогда $\phi(Y)$ – оптимальная оценка для $\theta$.\\
    Параметрическое семейство \textit{F} $= F_{\theta},\theta \in \Theta$ называется экспоненциальным, если плотность $f_{\theta}(x)$ имеет следующий вид
    \begin{center}
        $$\dissplaystyle f_{\theta}(x) = exp\{A(\theta) \cdot B(x) + C(\theta) + D(x)\}$$
    \end{center}
    Покажем, что распределение Максвелла принадлежит
    экспоненциальному семейству.
    \begin{center}
        $$\displaystyle f_{\theta}(x) = \frac{e^{\frac{x^2}{2\theta^2} \sqrt{\frac{2}{\pi}}x^2}}{\theta^3}$$
    \end{center}
    Преобразуем функцию плотности вероятности, преобразовав дробь выше в вид $exp\{\cdots\}$
    \begin{center}
        $$\displaystyle f_{\theta}(x) = exp\Big\{-\frac{1}{2\theta^2}x^2 +\ln\Big(\frac{1}{\theta^3}\Big) + \ln\Bigg(\sqrt{\frac{2}{\pi}}x^2\Bigg) \Big\}$$ 
    \end{center}
    
    Отсюда видим, что распределение Максвелла относится к полным
    экспоненциальным семействам, и $Y=T(X)= \sum_{i=1}^nB(x_i)=\sum_{i=1}^nx_i^2$
    полная и достаточная статистика для $\theta$. \\

    Тогда математическое ожидание от функци от данной оценки равное $\tau(\theta)$ будет оптимальной оценкой\\

    Если $X$ имеет распределения Максвелла с параметром $\theta$, то $\sum_{i=1}^n x_i^2$ имеет Гамма-распределение с параметром $2\theta^2$  и  степенями свободы $\frac{3}{2}n$
    $$\displaystyle \Bigg[\xi = \sum_{i=1}^n x_i^2\Bigg]=\Gamma(\frac{3}{2}n, \ 2\theta^2)$$
    $$F_{\sqrt{\xi}}(x)= P(\sqrt{\xi} \leq x) = P(\xi \leq x^2) = F_\xi (x^2)$$
    Функция вероятности, будет производной от функции $F_\xi(x^2)$
    $$\displaystyle \Big(F_\xi(x^2)\Big)'= 2xf(x^2)$$
    $$\displaystyle M(x^2)=\int_{0}^{\infty} 2x·x · (x^2)^{\frac{3}{2}n-1} e^{-\frac{x^2}{2\theta^2}}\frac{1}{(2\theta^2)^{\frac{3}{2}n} ?(\frac{3}{2}n)} dx = $$
    $$\displaystyle =\frac{2}{?(\frac{3}{2}n)}\int_0^\infty \Big(\frac{x^2}{2\theta^2}\Big)^{\frac{3}{2}n}e^{-\frac{x^2}{2\theta^2}}dx=$$
    $$\displaystyle =\Bigg|\frac{x^2}{2\theta^2},\ x = \theta\sqrt{2t}, \ dx = \frac{\theta}{\sqrt{2t}}dt\Bigg|= \frac{2\theta}{?(\frac{3}{2}n)}\int_0^\infty t^{\frac{3}{2}n} e^{-t} \frac{dt}{\sqrt{2t}} =$$
    $$\displaystyle =\frac{\sqrt{2}}{?(\frac{3}{2}n)}\int_0^\infty t^{\frac{3}{2}n-\frac{1}{2}}e^{-t}dt = \frac{\theta \sqrt{2}}{?(\frac{3}{2}n)}?(\frac{3}{2}n+\frac{1}{2})$$
    Для несмещенности оценки домножим на $\frac{?(\frac{3}{2}n)}{\sqrt{2}?(\frac{3}{2}n+\frac{1}{2})}$, получим, что функция 
    $$\displaystyle \frac{?(\frac{3}{2}n)}{\sqrt{2}?(\frac{3}{2}n+\frac{1}{2})}\sqrt{\sum_{i=1}^n x_i^2}$$
    Будет оптимальной оценкой для $\tau(\theta)=\theta$


    \chapter{Проверка статистических гипотез}
    \subsubsection{Критерий Согласия Колмогорова (Смирнова)}
    [\textit{\textbf{33} - А. Б. Чухно, Д. Б. Фомин, "Математическая Статистика. Курс лекций
    для студентов кафедры компьютерной
    безопасности " - \textbf{стр. 64}}]
    Статистика критерия определяется формулой:
    $$D_n = D_n(X) = \underset{x\in\mathbb{R}}{sup}\bigg|\widehat{F}_n(x)-F(x)\bigg|,$$
    где $D_n$ — это отклонение эмпирической функции распределения от теорети ческой функции распределения.\\
    Знаем, что $\widehat{F}_{n}$ является оптимальной, несмещенной и состоятельная
    оценкой для $F(x)$. Отсюда следует, что $D_{n}$ не должно «сильно» отклоняться
    от 0. Поэтому, по крайней мере при больших n, в тех случаях, когда гипотеза $H_0$ истинна, значение $D_n$ не должно существенно отклоняться от нуля.\\
    Отсюда следует, что критическую область критерия, основанного на статистике $T=D_n$,  следует задавать в виде $\tau_\alpha = \{t \geq t_\alpha\}$, т.е. большие значения $D_n$ надо интерпретировать как свидетельство против проверяемой гипотезы $H_0$.  Критическая граница $t_\alpha$ при заданном уровне значимости $\alpha$ рассчитывается при этом на основании теоремы Колмогорова. Положив $t_\alpha = \lambda_\alpha/\sqrt{n}$, где $K(\lambda_\alpha) = 1 - \alpha$, будем иметь
    $$P(D_n \in \tau_\alpha\big|H_0) = P(\lambda_\alpha\big|H_0 \leq \sqrt{n}D_n)\approx 1-K(\lambda_\alpha) = \alpha$$
    Тем самым критерий согласия Колмогорова формулируется следующим образом: при выбранном уровне значимости $\alpha$ число $\lambda_\alpha$ определено соотношение $K(\lambda_\alpha)= 1- \alpha$, то $H_0$
    $$H_0 \  \ принимается  \  \iff \sqrt{n}D_n < \lambda_\alpha$$ 
    

    \subsubsection{Критерий Согласия хи-квадрат}
    Пусть по наблюдения вектора частот $\underline{v}=(v_1,...,v_N)$ требуется проверить простую гипотезу $H_0$, $\underline{p}=\underline{p}^{\circ}\ \ \underline{p}^\circ = (p_1^\circ, ..., p_N^\circ)$ - заданный вероятностный вектор $(0<p_j^\circ < 1, \ j=1, ..., N,\ p_1^\circ+ ...+ p_N^\circ = 1)$. К. Пирсон в 1900 г. предложил использовать в качестве меры отклонение эмперических данных от гипотетических значений $\underline{p}^\circ$ меру хи-квадрат
    $$\overset{\circ}{X_n^2} = \overset{\circ}{X_n^2}(\underline{v}) = \sum_{j=1}^N \frac{(v_j - n p_j^\circ)^2}{n p_j^\circ}$$
    Данная статистика имеет распределения хи-квадрат с N-1 степенями свободы. Таким образом, классический критерий $\chi^2$ имеет следующий вид: пусть есть выборка объемом $n$ и наблюдавшиеся значения вектора частот $\underline{v} = (v_1,...,v_N)$; тогда при заданном уровне значимости $\alpha$
    $$H_0\ \  принимается \ \iff \overset{\circ}{X_n^2} \leq \chi_{1-\alpha, N-1}^2$$
    \\
    \\

    Для вышеописанных гипотез произведем проверку для всех сгенерированных ранее в ДЗ2 выборках. Проверка будет производиться на уровнях значимости $10\%, 5\%$ и $1\%$. Критические значения распределения Колмогорова позаимствуем с сайта $http://smc.edu.nstu.ru/krit\_kolm.htm$, они равны: $1.22, 1.36, 1.63$ соответственно. Данные уровни значимости будут применены для всех согласий. 

    \subsubsection{Критерий согласия Колмогорова (Смирнова) для сложной гипотезы (в
    условиях когда неизвестен параметр распределения)}
    Описанную выше методику проверки гипотезы о виде распределения наблюдаемой случайной величины можно распространить и на случай
    сложной гипотезы $H_0$. В этом случае используют тестовую статистику
    $$\hat{D}_n = \underset{-\infty<x<\infty}{sup}\Big|\hat{F}_n(x)-F(x;\hat{\theta}_n)\Big|$$
    где $\hat{\theta}_n$ - оценка максимального правдоподобия параметра $\theta_n$.

    \subsubsection{Критерий согласия хи-квадрат для сложной гипотезы (в условиях когда
    неизвестен параметр распределения)}
    В случае проверки сложной гипотезы используют тестовую статистику
    $$\hat{X}_n^2 = X_n^2\big(\hat{\theta}_n\big)=\sum_{j=1}^N \frac{(v_j - n p_j(\hat{\theta}_n))^2}{n p_j(\hat{\theta}_n)}$$
    где $\hat{\theta}_n$ оценка максимального правдоподобия $\theta_n$.
    $$H_0 \ \ принимается \ \iff \overset{\circ}{X_n^2} \leq \chi_{1-\alpha, N -1 - r}^2$$
    где $r$ количество параметров предполагаемого распределения.
    \\
    \\

    Для проверки же сложных гипотез воспользуемся оценками максимального правдоподобия из ДЗ3.\\
    \\
    В дальнейшем можно заметить, что для некоторых выборок в Распределении Максвелла и Биномиальном Распределении гипотеза $H_{0}$ отвергается. Исходя из этого, для критерия Хи-квадрат заметим, что:\\
        Если исходные данные представляют собой выборку из некоторого непрерывного распределения, то, применяя предварительно метод группировки наблюдений, приходят к рассмотрению дискретной схемы, в которых в качестве событий $A_j$ рассматриваются события $\{\xi \in \epsilon_j\}$, где $\epsilon_1,...,\epsilon_N$ - интервалы группировки.
    
    \section{Биномиальное распределение}
    Для проверки гипотез разработаем скрипты на ЯП Python и получим следующие результаты:
    \subsection{Проверка гипотезы о виде распределения}
    \subsubsection{Критерий Согласия Колмогорова (Смирнова)}
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Bin/KolmogBinSimple.png}}
    \end{center}\\
    \subsubsection{Критерий Согласия хи-квадрат}
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Bin/XiBinSimple.png}}
    \end{center}\\
    \subsubsection{Критерий согласия Колмогорова (Смирнова) для сложной гипотезы (в
    условиях когда неизвестен параметр распределения)}
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Bin/KolmogBinHard.png}}
    \end{center}\\
    \subsubsection{Критерий согласия хи-квадрат для сложной гипотезы (в условиях когда
    неизвестен параметр распределения)}
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Bin/XiBinHard.png}}
    \end{center}\\
    
    \subsection{Проверка гипотезы об однородности выборок}
    Из лекций знаем, что: Пусть $X = (x_{1}, \cdots, X_{n})$ из распределения $\mathfrak{L}(\xi)$ с неизвестной функцией распределения $F_{1}(x)$ и $Y = (Y_{1}, \cdots, Y_{n})$ из распределения $\mathfrak{L}(\nu)$ также с неизвестной функцией распределения $F_{2}(x)$. Гипотеза однородности формулируется следующим образом: $H_{0}: F_{1}(x) = F_{2}(x)$ и заключается в проверке гипотезы о том, что рассматриваются две выборки из одного и того же распределения.\\
    Предположим, что $Y_{1}, \cdots, Y_{n}$ - независимые, одинаково распределенные случайные величины ($Y_{i} \sim \mathcalc{R}[0,1]$). А $X_{1}, \cdots, X_{n}$ - выборка из биномиального распределения (функция распределения биномиального распределения имеет точки разрыва).\\
    \begin{center}
        $\displaystyle U_{i} = F(X_{i}-) + Y_{i}[F(x_{i}) - F(X_{i}-)]$ - случайная величина\\
        $\displaystyle F(X_{i}-) = lim_{z\downarrow0}F(X_{i} - z)$. Тогда случайная величина $\displaystyle U_{i} \sim \mathcalc{R}[0,1]$
    \end{center}
    В таком случае, можем применить критерий Смирнова: 
    Пусть $X, Y$ - две выборки объемов $n$ и $m$ соответственно $\hat{F}_{n}$ - э.ф.р, построенная по выборке $X$, $\hat{F}_{m}$ - э.ф.р., построенная по выборке $Y$ . Рассмотрим
    статистику:\\
    $$D_{n, m} = \underset{-\infty < x < +\infty}{sup} |\hat{F_n} \left(x\right) - \hat{F_m} \left(x\right)|$$
    в случае справедливости гипотезы однородности $H_0$ функции $\hat{F_n}\left(x\right)$ и $\hat{F_m}\left(x\right)$ оценивают одну и ту же неизвестную функцию распределения. Тем самым в этом случае (по крайней мере при больших $n$ и $m$) статистика $D_{n, m}$ не должна существенно отклоняться от нуля. Отсюда следует, что слишком большие значения этой статистики следует расценивать как свидетельство против гипотезы $H_0$. Критическую границу $t_{\alpha} \left(n, m\right)$ при заданном уровне значимости $\alpha$ находят на основании известного при гипотезе $H_0$ предельного распределения статистики $D_{n, m}$.
    При больших $n, m$ полагают $t_{\alpha} \left(n, m\right) = \sqrt{\frac{1}{n} + \frac{1}{m}} \lambda_{\alpha}$:
    $P\left(D_{n, m} > \sqrt{\frac{1}{n} + \frac{1}{m}}t_{\alpha} | H_0\right) = P\left(\sqrt{\frac{n m}{n+m}}D_{n, m} > t_{\alpha} | H_0\right) = \\ =$ при $m \longrightarrow \infty; n \longrightarrow \infty$ =  $ 1 - K\left(\lambda_{\alpha}\right) = \alpha,$
    $K\left(t\right)$ - функция распределения Колмогорова.\\
    Тогда:\\
    
    $$H_0 \ \ принимается \ \iff D_{n, m} \leq \lambda_{\alpha} \sqrt{\frac{1}{n} + \frac{1}{m}}$$

    \begin{center}
        \fbox{\includegraphics[scale=0.66]{HW4/Task2/Bin/Bin.png}}
    \end{center}\\
    
    \newpage
    \section{Распределение Максвелла}
    \subsection{Проверка гипотезы о виде распределение}
    \subsubsection{Критерий Согласия Колмогорова (Смирнова)}
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Max/KolmogMaxSimple.png}}
    \end{center}\\
    \subsubsection{Критерий Согласия хи-квадрат}
    $N = 50$\\
    Интервалы разбиения вычисляются следующим образом:\\
    $l = list(sorted(selection))\\
    delta = (math.floor(l[-1]) + 1 - round(l[0])) / N\\
    leftborder, rightborder = math.floor(l[0]), math.floor(l[0]) + delta$\\
    где $selection$ - выборка
    
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Max/XiMaxSimple.png}}
    \end{center}\\
    \subsubsection{Критерий согласия Колмогорова (Смирнова) для сложной гипотезы (в
    условиях когда неизвестен параметр распределения)}
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Max/KolmogMaxhard.png}}
    \end{center}\\
    \subsubsection{Критерий согласия хи-квадрат для сложной гипотезы (в условиях когда
    неизвестен параметр распределения)}
    $N = 50$\\
    Интервалы разбиения задаются аналогично разделу "Критерий согласия хи-квадрат", описанному выше.
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW4/Task1/Max/XiMaxHard.png}}
    \end{center}\\
    \subsection{Проверка гипотезы об однородности выборок}
    Для проверки используем критерий однородности Смирнова (описан выше в пункте 4.1.2)
    \begin{center}
        \fbox{\includegraphics[scale=0.66]{HW4/Task2/Max/image.png}}
    \end{center}\\

    \chapter{Различение статистических гипотез}
    Пусть имеется выборка $X = (x_{1},\cdots,X_{n})$ из распределения $\mathcal{L}(\xi)$.\\
    Рассмотрим две гипотезы:\\
    $\displaystyle H_{0}: \xi \sim \mathcal{L}(\theta_{0})$ - основная гипотеза\\
    $\displaystyle H_{1}: \xi \sim \mathcal{L}(\theta{1}), (\theta_{1} > \theta_{0})$ - альтернативная гипотеза\\
    Набор из двух простых гипотез можно представить в виде параметрической гипотезы:\\
    Пусть $\displaystyle \Theta = \{0,1\}$ и $\displaystyle F_{\theta}(x) = (1 - \theta)F_{0}(x) + F_{1}(x)$\\
    В случае параметрических гипотез функцию мощности можно переписать в виде\\
    $W(\theta) = W(\theta, \mathfrak{X})_{1, \alpha} = P_{\theta}(X \in \mathfrak{X}_{1, \alpha})$\\
    Понятие ошибок:\\
    \begin{itemize}
        \item \textbf{Ошибка первого рода (отвергаем истину)}\\
        $\displaystyle P(x \in \mathfrak{X}_{1}|H_{0}) = \alpha$
        \item \textbf{Ошибка второго рода (принимаем ложь за истину)}\\
        $\displaystyle P(x \in \mathfrak{X}_{0}|H_{1}) = \beta$
    \end{itemize}
    [\textit{\textbf{34} - А. Б. Чухно, Д. Б. Фомин, "Математическая Статистика. Курс лекций
    для студентов кафедры компьютерной
    безопасности " - \textbf{стр. 62}}] Функцией мощности критерия $W$ назовем функционал на множестве допустимых распределений $\mathcal{F}$ и выборке $X$.\\
    \begin{center}
        $\displaystyle W(F_{X)} = W(F_{X}; \mathfrak{X}_{1,\alpha})=P(X \in \mathfrak{X}_{1,\alpha}|F_{X}),$
    \end{center}
    где $\displaystyle P(x \in \mathfrak{X}_{1,\alpha}|F_{X})$ - вероятность попасть в $\mathfrak{X}_{1,\alpha}$, если $F_{X}$ - истинное распределение.\\
    Через функцию мощности критерия легко можно выразить вероятности ошибок первого и второго рода:\\
    $\displaystyle \alpha = W(F_{0, X}) = W(\theta_{0}, \mathfrak{X}_{1,\alpha})\\
    \beta = 1 - W(F_{1, X}) = 1 - W(\theta_{1}, \mathfrak{X}_{1,\alpha})$\\
    Параметрический критерий, минимизирующий ошибку 2 рода при заданной ошибке 1 рода называется наиболее мощным критерием уровнем значимости  $\alpha$.\\
    Критическую область можно построить следующим образом: множество $\mathfrak{X}_{1,\alpha}$ состоит из таких $\overline{x}$ для которых правдоподобие $L(\overline{x}, \theta_{1})$ будет больше правдоподобия $L(\overline{x}, \theta_{0})$\\
    [[\textit{\textbf{35} - А. Б. Чухно, Д. Б. Фомин, "Математическая Статистика. Курс лекций
    для студентов кафедры компьютерной безопасности" - \textbf{стр. 76}}]] Функция, имеющая вид
    \begin{center}
        $\displaystyle l(\overline{x}) = \frac{L(\overline{x},\theta_{1})}{L(\overline{x}, \theta_{0})} = \frac{\prod_{i=1}^{n}f_{1}(x_{i})}{\prod_{i=1}^{n}f_{0}(x_{i})}$
    \end{center}
    называется функцией отношения правдоподобия.\\
    Выберем некоторую границу $c$. Если $l(\overline{x}) \geq c$, то принимаем $H_{1}$, иначе - $H_{0}$\\
    Критическим множеством критерия Неймана-Пирсона называется множество $\mathfrak{X}_{1, \alpha}^{*}$ имеющее вид:

    $$\mathfrak{X}_{1, \alpha}^{*} = \{\overline{x} \in \mathfrak{X}: l\left(\overline{x}\right) \geq c_{\alpha}\},$$
    
    где $c_{\alpha}$ такое, что ошибка 1 рода равна $\alpha$.
    
    Для данного множества верно
    
    $$W\left(\theta_0, \mathfrak{X}_{1, \alpha}^{*}\right) = P_{0} \left(\overline{x} \in \mathfrak{X}_{1, \alpha}^{*}\right) = \alpha$$
    
    Также стоит отметить, что Лемма Неймана-Пирсона говорит о том, что критическая область $\mathfrak{X}_{1, \alpha}^{*}$ задает наиболее мощный критерий для гипотезы $H_0$ относительно альтернативы $H_1$ среди всех критериев с уровнем значимости $\alpha$. Кроме того, данный критерий является несмещенным.
    \section{Биномиальное Распределение}
    Рассмотрим две гипотезы:
    \begin{center}
        $\displaystyle H_{0}: \xi \sim Bin(87, \theta_{0}), \theta_{0} = 0.6$\\
    $\displaystyle H_{1}: \xi \sim Bin(87, \theta_{1}), \theta_{1} > \theta_{0} = 0.62$\\
    \end{center}
    
    
    \subsection{Вычисление функции отношения правдоподобия}
    $\displaystyle l(\overline{x}) = \frac{\prod_{i=0}^{n}f_{1}(x_{i})}{\prod_{i=0}^{n}f_{0}(x_{i})} = \frac{\prod_{i=0}^{n}\binom{n}{x_{i}}\theta_{1}^{x_{i}}(1 - \theta_{1})^{n - x_{i}}}{\prod_{i=0}^{n}\binom{n}{x_{i}}\theta_{0}^{x_{i}}(1-\theta_{0})^{n-x_{i}}} = \prod_{i=0}^{n}\Bigg(\frac{\theta_{1}}{\theta_{0}}\Bigg)^{x_{i}}\Bigg(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Bigg)^{n - x_{i}} = \Bigg(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Bigg)^{n^{2}}\prod_{i=0}^{n}\Bigg(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Bigg)^{x_{i}} = \Bigg(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Bigg)^{n^{2}}e^{ln\Big(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Big)\sum_{i=0}^{n}x_{i}}$
    \begin{center}
        $\displaystyle \Bigg(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Bigg)^{n^{2}}e^{ln\Big(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Big)\sum_{i=0}^{n}x_{i}} \geq c$
    \end{center}
    \begin{center}
        $\displaystyle e^{ln\Big(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Big)\sum_{i=0}^{n}x_{i}} \geq c\Bigg(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Bigg)^{n^{2}}$
    \end{center}
    Логарифмируем обе части неравенства
    \begin{center}
        $\displaystyle ln\Big(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Big)\sum_{i=0}^{n}x_{i} \geq ln(c) + n^{2}ln\Big(\frac{1 - \theta_{0}}{1 - \theta_{1}}\Big)$
    \end{center}
    \begin{center}
        $\displaystyle \sum_{i=0}^{n}x_{i} \geq \frac{ln(c) + n^{2}ln\Big(\frac{1 - \theta_{0}}{1 - \theta_{1}}\Big)}{ln\Big(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Big)}$
    \end{center}
    Обозначим $c_{\alpha}$ следующим образом:
    \begin{center}
        $\displaystyle c_{\alpha} = \frac{ln(c) + n^{2}ln\Big(\frac{1 - \theta_{0}}{1 - \theta_{1}}\Big)}{ln\Big(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Big)}$
    \end{center}
    Критическая область: $\displaystyle \sum_{i=0}^{n}x_{i}\geq c_{\alpha}$ (Отвергаем $H_{0}$)\\
    Тогда $\displaystyle \alpha = P(H_{1}|H_{0}) = P_{0}(\sum_{i=0}^{n}x_{i}\geq c_{\alpha}) = 1 - P_{0}\Big(\sum_{i=0}^{n} < c_{\alpha}\Big)$\\
    Также заметим, что $\displaystyle n\cdot\overline{X} = \sum_{i=0}^{n}x_{i} = \xi_{0} \sim Bin(n\cdot87, \theta)$\\
    Таким образом, ошибка первого рода вычисляется следующим образом: \\$\displaystyle \alpha = 1 - P(\xi_{0} < c_{\alpha});$\\
    Ошибка второго рода вычисляется: $\beta = P(H_{0}|H_{1}) = 1 - P(H_{1}|H_{0}) = 1 - P_{1}\Big(\sum_{i=1}^{n}x_{i} \geq c_{\alpha}\Big) = 1 - \Big(1 - P_{1}\Big(\sum_{i=1}^{n}x_{i}<c_{\alpha}\Big)\Big) = \\ = P(\xi_{1}<c_{\alpha}); \xi_{1} \sim Bin(87\cdot n, \theta_{1})$
    
    \subsection{Вычисление критической области}
    Вычислим $c_{\alpha}$ для всех представленнных в данной работе объемов выборок при помощи скрипта на ЯП Python, возьмем $\alpha = 0.01$. Таким образом нам надо вычислить квантиль уровня 0.99 для $F_{\xi_{0}}$\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW5/Bin/Bin.png}}
    \end{center}\\

    Вычислим ошибку второго рода для известных $\alpha = 0.01$ и $c_{\alpha}$
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW5/Bin/Bin2.png}}
    \end{center}\\
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW5/Bin/first.py}\\
    

    \subsection{Вычисление минимального необходимого материала при фиксации минимального возможного значения ошибок первого и второго рода}
    Критическая область критерия Неймана-Пирсона для рассматриваемого случая выглядит следующим образом:\\
    \begin{center}
        $\displaystyle \Big(\frac{1-\theta_{1}}{1 - \theta_{0}}\Big)^{n^{2}}exp\Bigg[ln\Bigg(\frac{\theta_{1}(1 - \theta_{0})}{\theta_{0}(1 - \theta_{1})}\Bigg)\sum_{i=1}^{n}x_{i}\Bigg]$
    \end{center}
    Что эквивалентно следующему:\\
    \begin{center}
        $\displaystyle\sum_{i=1}^{n}x_{i}\geq\frac{ln(c) + n^{2}ln\Big(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Big)}{ln\Big(\frac{\theta_{1}}{\theta_{0}}\Big) - ln\Big(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Big)}$
    \end{center}
    Заметим:
    \begin{center}
        $\displaystyle \sum_{i=1}^{n}x_{i} \sim Bin(nk, p); k = 87\\$
        $Bin(nk, \theta) \approx N(nk\theta, nk\theta(1 - \theta))$
    \end{center}
    Произведем нормализацию для $\sum_{i=1}^{n}x_{i}$, используя сдвиг и деление:
    \begin{center}
        $\displaystyle \frac{\sum_{i=1}^{n}x_{i} - nk\theta_{0}}{\sqrt{nk\theta_{0}(1 - \theta_{0})}} \sim N(0,1)$
    \end{center}
    Обозначим:
    \begin{center}
        $\displaystyle t = \frac{\frac{ln(c) + n^{2}ln\Big(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Big)}{ln\Big(\frac{\theta_{1}}{\theta_{0}}\Big) - ln\Big(\frac{1 - \theta_{1}}{1 - \theta_{0}}\Big)} - nk\theta_{0}}{\sqrt{nk\theta_{0}(1 - \theta_{0})}}$
    \end{center}
    Тогда ошибка первого рода:
    \begin{center}
        $\displaystyle \alpha = P_{0}\Bigg(\frac{\sum_{i=1}^{n}x_{i} - nk\theta_{0}}{\sqrt{nk\theta_{0}(1 - \theta_{0})}}\geq t\Bigg) = P_{0}(\xi \geq t) = 1 - P_{0}(\xi \leq t) = \Phi(-t)$
    \end{center}
    $\Phi$ - это функция стандартного нормального распределния. Пусть $t_{\alpha}$ - решение написанного выше уравнения от переменной $t$. То есть $\alpha = \Phi(t_{\alpha})$ - такое решение всегда существует ввиду непрерывности функции
    $\Phi$.
    Аналогично, при фиксированной ошибке первого рода выпишем мощность
    критерия:\\
    $\displaystyle 1 - \beta = P_{1}\Bigg(\frac{\sum_{i=1}^{n}x_{i} - nk\theta_{0}}{\sqrt{nk\theta_{0}(1 - \theta_{0})}}\geq t_{\alpha}\Bigg) = \\ 
    = P_{1}\Big(\sum_{i=1}^{n}x_{i} - kn\theta_{0} \geq t_{\alpha}\sqrt{kn\theta_{0}(1 - \theta_{0})}\Big) = \\ = 
    P_{1}\Big(\sum_{i=1}^{n}x_{i} - kn\theta_{0} \geq t_{\alpha}\sqrt{kn\theta_{0}(1 - \theta_{0})} + kn(\theta_{0} - \theta_{1})\Big) = \\ = P_{1}\Bigg(\frac{\sum_{i=1}^{n}x_{i} - nk\theta_{1}}{\sqrt{nk\theta_{1}(1 - \theta_{1})}} \geq \frac{t_{\alpha}\sqrt{nk\theta_{0}(1 - \theta_{0})} + nk(\theta_{0} - \theta_{1})}{\sqrt{nk\theta_{1}(1 - \theta_{1})}}\Bigg) = \\ = \Phi\Bigg(-\frac{t_{\alpha}\sqrt{nk\theta_{0}(1 - \theta_{0})} + nk(\theta_{0} - \theta_{1})}{\sqrt{nk\theta_{1}(1 - \theta_{1})}}\Bigg)$\\
    Откуда\\
    $\displaystyle \beta = \Phi\Bigg(\frac{t_{\alpha}\sqrt{nk\theta_{0}(1 - \theta_{0})} + nk(\theta_{0} - \theta_{1})}{\sqrt{nk\theta_{1}(1 - \theta_{1})}}\Bigg)$\\
    Пусть $\Phi(t_{\beta}) = \beta$, тогда\\
    $\displaystyle t_{\beta} = \frac{t_{\alpha}\sqrt{nk\theta_{0}(1 - \theta_{0})} + nk(\theta_{0} - \theta_{1})}{\sqrt{nk\theta_{1}(1 - \theta_{1})}} \Rightarrow \\
    \Rightarrow t_{\beta} = t_{\alpha}\sqrt{\frac{\theta_{0}(1 - \theta_{0})}{\theta_{1}(1 - \theta_{1})}} + \sqrt{nk}\Bigg(\frac{\theta_{0} - \theta_{1}}{\sqrt{\theta_{1}(1 - \theta_{1})}}\Bigg)$\\
    Следовательно, необходимый объем выборки оценивается по формуле:\\
    $\displaystyle n = \Bigg[\Bigg(\Bigg(t_{\beta} - t_{\alpha}\sqrt{\frac{\theta_{0}(1 - \theta_{0})}{\theta_{1}(1 - \theta_{1})}}\Bigg)\frac{\sqrt{\theta_{1}(1 - \theta_{1})}}{\sqrt{k}(\theta_{0} - \theta_{1})}\Bigg)^{2}\Bigg]$\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW5/Bin/BinLast.png}}
    \end{center}\\

    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW5/Bin/third.py}\\
    
    
    \section{Распределение Максвелла}
    Рассмотрим две гипотезы:
    
    $$H_0 : \xi \sim Maxwell(\theta_0), \ \theta_0 = 3$$
    $$H_1 : \xi \sim Maxwell(\theta_1), \ \theta_1 = 3.5$$
    \subsection{Вычисление функции отношения правдоподобия}
$$l\left(\overline{x}\right) = \frac{L\left(\overline{x}, \theta_1\right)}{L\left(\overline{x}, \theta_0\right)} = \frac{\prod\limits_{i=0}^{n} f_{1} \left(x_i\right)}{\prod\limits_{i=0}^{n} f_{0} \left(x_i\right)} = \frac{\prod\limits_{i=1}^{n} \sqrt{\frac{2}{\pi}}x_i^2 e^{-\sum\limits_{i=0}^{n}\frac{x_i^2}{2\theta_1^2}}}{\theta_1^{3n}} \frac{\theta_0^{2n}}{\prod\limits_{i=1}^{n}\sqrt{\frac{2}{\pi}}x_i^2 e^{-\sum\limits_{i=0}^{n}\frac{x_i^2}{2\theta_0^3}}} =$$
$$= \left(\frac{\theta_0}{\theta_1}\right)^{3n} e^{\sum\limits_{i=0}^{n}x_i^2 \left(\frac{1}{2\theta_0^2} - \frac{1}{2\theta_1^2}\right)} \geq c$$

$$e^{\sum\limits_{i=0}^{n}x_i^2 \left(\frac{1}{2\theta_0^2} - \frac{1}{2\theta_1^2}\right)} \geq c \left(\frac{\theta_0}{\theta_1}\right)^{3n}$$

$$\sum\limits_{i=0}^{n}x_i^2 \left(\frac{1}{2\theta_0^2} - \frac{1}{2\theta_1^2}\right) \geq ln c + 3n ln\left(\frac{\theta_1}{\theta_0}\right)$$

$$\sum\limits_{i=0}^{n}x_i^2 \geq \frac{\left(ln c + 3n ln\left(\frac{\theta_1}{\theta_0}\right)\right) 2 \theta_0^2 \theta_1^2}{\theta_1^2 - \theta_0^2}$$

Пусть

$$c_{\alpha} = \frac{\left(ln c + 3n ln\left(\frac{\theta_1}{\theta_0}\right)\right) 2 \theta_0^2 \theta_1^2}{\theta_1^2 - \theta_0^2}$$

Заметим, что

$$\sum\limits_{i=1}^{n} x_i^2 = \xi_0 \sim \Gamma\left(\frac{3n}{2}, 2\theta^2\right)$$

Критическая
область

$$\sum\limits_{i=0}^{n}x_i^2 \geq c_{\alpha} \Longleftrightarrow \text{отвергаем} \;\; H_0$$

Тогда

$$\alpha = P_{\theta_0}\left(\sum\limits_{i=0}^{n}x_i^2 \geq c_{\alpha}\right) = 1 - P_{\theta_0}\left(\sum\limits_{i=0}^{n}x_i^2 < c_{\alpha}\right) = 1 - P_{\theta_0}\left(\xi_0 < c_{\alpha}\right)$$

$$\beta = 1 - P_{\theta_1}\left(\sum\limits_{i=0}^{n}x_i^2 \geq c_{\alpha}\right) = P_{\theta_1}\left(\sum\limits_{i=0}^{n}x_i^2 < c_{\alpha}\right) = P_{\theta_1}\left(\xi_0 < c_{\alpha}\right)$$
    \subsection{Вычисление критической области}
    Вычислим $c_{\alpha}$ для всех представленнных в данной работе объемов выборок
при помощи скрипта на ЯП Python, возьмем $\alpha$ = 0.01. Т.е. для этого необходимо вычислить квантиль
    уровня 0.99 дл $F_{\xi_0}(x)$\\
    Написав скрипт на ЯП Python, получаем, что:\\
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW5/Max/alpha.png}}
    \end{center}
    Вычислим ошибку второго рода для известных $\alpha$ = 0.01 и $c_{\alpha}$
    \begin{center}
        \fbox{\includegraphics[scale=1]{HW5/Max/beta.png}}
    \end{center}
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW5/Max/main.py}\\
    

        \subsection{Вычисление минимального необходимого количества материала при фиксации минимального возможного значения ошибокпервого и второго рода}
    Критическая область критерия Неймана-Пирсона для рассматриваемого случая выглядит следюущим образом:
    
    $$ \left(\frac{\theta_0}{\theta_1}\right)^{3n} e^{\sum\limits_{i=0}^{n}x_i^2 \left(\frac{1}{2\theta_0^2} - \frac{1}{2\theta_1^2}\right)} \geq c$$
    
    Эквивалентно:
    
    $$\sum\limits_{i=0}^{n}x_i^2 \geq \frac{\left(ln c + 3n ln\left(\frac{\theta_1}{\theta_0}\right)\right) 2 \theta_0^2 \theta_1^2}{\theta_1^2 - \theta_0^2}$$
    
    Заметим, что
    
    $$\sum\limits_{i=0}^{n}x_i^2 \sim \Gamma\left(\frac{3}{2}n, 2 \theta^2\right)$$
    
    $$\Gamma\left(\frac{3}{2}n, 2 \theta^2\right) \approx N\left(3 n \theta^2, 6 n \theta^2\right)$$
    
    Нормализуем $\sum\limits_{i=0}^{n}x_i^2$:
    
    $$\frac{\sum\limits_{i=0}^{n}x_i^2 - 3 n \theta_0^2}{ \sqrt{6n} \theta_0^2} \sim N\left(0, 1\right)$$
    
    
    
    
    Обозначим
    
    $$t = \frac{\frac{\left(ln c + 3n ln\left(\frac{\theta_1}{\theta_0}\right)\right) 2 \theta_0^2 \theta_1^2}{\theta_1^2 - \theta_0^2} - 3 n \theta_0^2}{ \sqrt{6n} \theta_0^2}$$
    
    Тогда ошибка первого рода:
    
    $$\alpha = P_0\left(\frac{\sum\limits_{i=0}^{n}x_i^2 - 3 n \theta_0^2}{ \sqrt{6n} \theta_0^2} \geq t\right) = \Phi\left(-t\right)$$
    
    Пусть $t_{\alpha}$ - решение написанного выше уравнения от переменной $t$. Т.е. $\alpha = \Phi\left(t_{\alpha}\right)$ - такое решение всегда существует в виду непрерывности функции $\Phi$.
    
    Аналогичным образом, при фиксированной ошибке первого рода выпишем мощность критерия:
    
    $$1 - \beta = P_1\left(\frac{\sum\limits_{i=0}^{n}x_i^2 - 3 n \theta_0^2}{ \sqrt{6n} \theta_0^2} \geq t_{\alpha}\right) = P_1\left(\frac{\sum\limits_{i=0}^{n}x_i^2
    - 3 n \theta_1^2}{ \sqrt{6n} \theta_1^2} \geq \frac{t_{\alpha}\sqrt{6n} \theta_0^2 + 3n \left(\theta_0^2-\theta_1^2\right)}{\sqrt{6n}\theta_1^2}\right) =$$
    $$= \Phi\left(-\frac{t_{\alpha} \sqrt{6n} \theta_0^2 + 3n \left(\theta_0^2-\theta_1^2\right)}{\sqrt{6n}\theta_1^2}\right)$$
    
    Откуда
    
    $$\beta = \Phi\left(\frac{t_{\alpha} \sqrt{6n} \theta_0^2 + 3n \left(\theta_0^2-\theta_1^2\right)}{\sqrt{6n}\theta_1^2}\right)$$
    
    Пусть
    
    $$\Phi\left(t_{\beta}\right) = \beta$$
    
    Тогда
    
    $$t_{\beta} = \frac{t_{\alpha} \sqrt{6n} \theta_0^2 + 3n \left(\theta_0^2-\theta_1^2\right)}{\sqrt{6n}\theta_1^2}$$
    
    $$t_{\beta} = t_{\alpha} \left(\frac{\theta_0}{\theta_1}\right)^2 + \frac{3\sqrt{n}}{\sqrt{6}}\frac{\theta_0^2-\theta_1^2}{\theta_1^2}$$
    
    Необходимый объем выборки оценивается по формуле:
    
    $$n = \Bigg\lceil\left(\left(t_{\beta} - t_{\alpha} \left(\frac{\theta_0}{\theta_1}\right)^2\right) \frac{\sqrt{6}\theta_1^2}{3\left(\theta_0^2-\theta_1^2\right)}\right)^2\Bigg\rceil$$
    
    Продемонстрируем вычисление необходимого объема выборки по фиксированным ошибкам первого и второго рода:

    \begin{center}
        \fbox{\includegraphics[scale=1]{HW5/Max/Task2.png}}
    \end{center}
    \textbf{Листинг кода:}
    \lstinputlisting[language=Python]{HW5/Max/task1.py}\\

    
    
    
\end{document}